<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><title>tansformer 从入门到上手只需要看这一篇 | Blog of Shane Hu</title><noscript>Please enable JavaScript to view the site</noscript><link rel="icon" href="/img/pwa/favicon.png"><!-- index.css--><link rel="stylesheet" href="/css/index.css?v=3.0.14"><!-- inject head--><link rel="canonical" href="https://hs3434.github.io/hs3434.github.io/2025/03/03/transformer1/index.html"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css"><!-- aplayer--><!-- swiper--><!-- fancybox ui--><!-- katex--><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css"><!-- Open Graph--><meta name="description" content="tansformer 从入门到上手只需要看这一篇 tansformer 架构解析 架构总览 词嵌入层 Embeddings 输入 token 的索引处在维度为词表大小 d_vocab 的空间里，每个 token 要经过词嵌入层变成维度为 d_model 的特征向量。 12345678class E"><!-- pwa--><meta name="apple-mobile-web-app-capable" content="Blog of Shane Hu"><meta name="theme-color" content="var(--efu-main)"><meta name="apple-mobile-web-app-status-bar-style" content="var(--efu-main)"><link rel="bookmark" href="/img/pwa/favicon.png"><link rel="apple-touch-icon" href="/img/pwa/favicon.png" sizes="180x180"><script>console.log(' %c Solitude %c ' + '3.0.14' + ' %c https://github.com/everfu/hexo-theme-solitude',
    'background:#35495e ; padding: 1px; border-radius: 3px 0 0 3px;  color: #fff',
    'background:#ff9a9a ; padding: 1px; border-radius: 0 3px 3px 0;  color: #fff',
    'background:unset ; padding: 1px; border-radius: 0 3px 3px 0;  color: #fff')
</script><script>let mdate = "7-7,9-18,12-13";
mdate = (mdate.split(","));
let ndate = new Date();
for (let i of mdate) {
    if (i === (ndate.getMonth()+1) + "-" + (ndate.getDate())) {
        document.documentElement.classList.add('memorial');
    }
}
</script><script>(()=>{
        const saveToLocal = {
            set: function setWithExpiry(key, value, ttl) {
                if (ttl === 0)
                    return
                const now = new Date()
                const expiryDay = ttl * 86400000
                const item = {
                    value: value,
                    expiry: now.getTime() + expiryDay
                }
                localStorage.setItem(key, JSON.stringify(item))
            },
            get: function getWithExpiry(key) {
                const itemStr = localStorage.getItem(key)

                if (!itemStr) {
                    return undefined
                }
                const item = JSON.parse(itemStr)
                const now = new Date()

                if (now.getTime() > item.expiry) {
                    localStorage.removeItem(key)
                    return undefined
                }
                return item.value
            }
        };
        window.utils = {
            saveToLocal: saveToLocal,
            getCSS: (url, id = false) => new Promise((resolve, reject) => {
              const link = document.createElement('link')
              link.rel = 'stylesheet'
              link.href = url
              if (id) link.id = id
              link.onerror = reject
              link.onload = link.onreadystatechange = function() {
                const loadState = this.readyState
                if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
                link.onload = link.onreadystatechange = null
                resolve()
              }
              document.head.appendChild(link)
            }),
            getScript: (url, attr = {}) => new Promise((resolve, reject) => {
              const script = document.createElement('script')
              script.src = url
              script.async = true
              script.onerror = reject
              script.onload = script.onreadystatechange = function() {
                const loadState = this.readyState
                if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
                script.onload = script.onreadystatechange = null
                resolve()
              }

              Object.keys(attr).forEach(key => {
                script.setAttribute(key, attr[key])
              })

              document.head.appendChild(script)
            }),
            addGlobalFn: (key, fn, name = false, parent = window) => {
                const globalFn = parent.globalFn || {}
                const keyObj = globalFn[key] || {}

                if (name && keyObj[name]) return

                name = name || Object.keys(keyObj).length
                keyObj[name] = fn
                globalFn[key] = keyObj
                parent.globalFn = globalFn
            },
            addEventListenerPjax: (ele, event, fn, option = false) => {
              ele.addEventListener(event, fn, option)
              utils.addGlobalFn('pjax', () => {
                  ele.removeEventListener(event, fn, option)
              })
          },
        }
    })()</script><!-- theme--><script>initTheme = () => {
    let isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const cachedMode = utils.saveToLocal.get('theme');
    if (cachedMode === undefined) {
        const nowMode =
            isDarkMode ? 'dark' : 'light'
        document.documentElement.setAttribute('data-theme', nowMode);
    } else {
        document.documentElement.setAttribute('data-theme', cachedMode);
    }
    typeof rm === 'object' && rm.mode(cachedMode === 'dark' && isDarkMode)
}
initTheme()</script><!-- global head--><script>const GLOBAL_CONFIG = {
    root: '/',
    algolia: undefined,
    localsearch: undefined,
    runtime: '2024-06-03 00:00:00',
    lazyload: {
        enable: true,
        error: '/img/error_load.avif'
    },
    copyright: false,
    highlight: {"limit":200,"expand":true,"copy":true,"syntax":"highlight.js"},
    randomlink: false,
    lang: {"theme":{"dark":"Dark","light":"Light"},"copy":{"success":"Copied","error":"Copy failed"},"backtop":"Back to top","time":{"day":" days ago","hour":" hours ago","just":"just","min":" minutes ago","month":" months ago"},"day":" days","f12":"Developer mode is turned on, please follow the GPL.","totalk":"You don't need to delete blank lines, just type in your comments.","barrage":{"title":"Hot"}},
    aside: {
        state: {
            morning: "✨ 早上好，新的一天开始了",
            noon: "🍲 午餐时间",
            afternoon: "🌞 下午好",
            night: "早点休息",
            goodnight: "晚安 😴",
        },
        witty_words: [],
        witty_comment: {
            prefix: 'Long time no see, ',
            back: 'Welcome back again,',
        },
    },
    covercolor: {
        enable: false
    },
    comment: {"avatar":"https://gravatar.com/avatar","commentBarrage":true},
    lightbox: 'null',
    right_menu: false,
    translate: {"translateDelay":0,"defaultEncoding":2},
    lure: false,
    expire: false,
};</script><!-- page-config head--><script id="config-diff">var PAGE_CONFIG = {
    is_post: true,
    is_page: false,
    is_home: false,
    page: '',
    toc: true,
    comment: true,
    ai_text: false,
    color: false,
}</script><meta name="generator" content="Hexo 7.3.0"></head><body id="body"><!-- universe--><canvas id="universe"></canvas><!-- background img--><!-- loading--><!-- console--><!-- sidebar--><div id="sidebar" style="zoom: 1;"><div id="menu-mask" style="display: none;"></div><div id="sidebar-menus"><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Archives</div><div class="length-num">18</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">7</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">27</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/recentcomments/"><div class="headline"> Comment</div><div class="length-num" id="waline_allcount"><i class="solitude fas fa-spinner fa-spin"></i></div><script pjax="pjax">(async () => {
  await fetch('/api/comment?type=recent&count=-1', {method: 'GET'}).then(async res => res.json())
    .then(async data => {
      document.querySelector('#waline_allcount').innerHTML = data.length
    })
})()</script></a></div></div></div><span class="sidebar-menu-item-title">Function</span><div class="sidebar-menu-item"><span class="darkmode_switchbutton menu-child" onclick="sco.switchDarkMode()"><i class="solitude fas fa-circle-half-stroke"></i><span>Display mode</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>文库</span></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="solitude  fas fa-folder-closed"></i><span>全部文章</span></a></li><li><a class="site-page child" href="/categories/"><i class="solitude  fas fa-clone"></i><span>全部分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="solitude  fas fa-tags"></i><span>全部标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>关于</span></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="solitude  st-contacts-fill"></i><span>自我介绍</span></a></li></ul></div></div><span class="sidebar-menu-item-title">Tags</span><div class="card-tags"><div class="card-tag-cloud"><a href="/tags/web/">web<sup>1</sup></a><a href="/tags/http/">http<sup>1</sup></a><a href="/tags/dns/">dns<sup>2</sup></a><a href="/tags/debian/">debian<sup>1</sup></a><a href="/tags/53port/">53port<sup>1</sup></a><a href="/tags/socks5/">socks5<sup>1</sup></a><a href="/tags/Dante/">Dante<sup>1</sup></a><a href="/tags/vscode-python-conda/">vscode python conda<sup>1</sup></a><a href="/tags/bind9/">bind9<sup>1</sup></a><a href="/tags/conda/">conda<sup>1</sup></a><a href="/tags/algebra/">algebra<sup>1</sup></a><a href="/tags/mathematics/">mathematics<sup>1</sup></a><a href="/tags/hexo/">hexo<sup>2</sup></a><a href="/tags/Fonts/">Fonts<sup>1</sup></a><a href="/tags/OpenWRT/">OpenWRT<sup>1</sup></a><a href="/tags/server/">server<sup>1</sup></a><a href="/tags/vps/">vps<sup>1</sup></a><a href="/tags/theme/">theme<sup>1</sup></a><a href="/tags/ssl/">ssl<sup>1</sup></a><a href="/tags/AI-pytorch/">AI pytorch<sup>1</sup></a><a href="/tags/jottings/">jottings<sup>2</sup></a><a href="/tags/python/">python<sup>2</sup></a><a href="/tags/mysql/">mysql<sup>2</sup></a><a href="/tags/api/">api<sup>2</sup></a><a href="/tags/udesk/">udesk<sup>2</sup></a><a href="/tags/ydns/">ydns<sup>1</sup></a><a href="/tags/ddns/">ddns<sup>1</sup></a></div></div></div></div><!-- keyboard--><!-- righhtside--><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav class="show" id="nav"><div id="nav-group"><div id="blog_name"><a id="site-name" href="/" title="Back to home"><span class="title">主页</span><i class="solitude fas fa-home"></i></a></div><div id="page-name-mask"><div id="page-name"><a id="page-name-text" onclick="sco.toTop()">tansformer 从入门到上手只需要看这一篇</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>文库</span></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="solitude  fas fa-folder-closed"></i><span>全部文章</span></a></li><li><a class="site-page child" href="/categories/"><i class="solitude  fas fa-clone"></i><span>全部分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="solitude  fas fa-tags"></i><span>全部标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>关于</span></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="solitude  st-contacts-fill"></i><span>自我介绍</span></a></li></ul></div></div></div><div id="nav-left"></div><div id="nav-right"><div class="nav-button" id="nav-totop" onclick="sco.toTop()"><a class="totopbtn"><i class="solitude fas fa-arrow-up"></i><span id="percent">0</span></a></div><div id="toggle-menu"><a class="site-page"><i class="solitude fas fa-bars"></i></a></div></div></div></nav><div class="coverdiv" id="coverdiv"><img class="nolazyload" id="post-cover" src="/2025/03/03/transformer1/image.png" alt="tansformer 从入门到上手只需要看这一篇"></div><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original" title="This article is a Original article, pay attention to the copyright.">Original</a><span class="post-meta-categories"><a class="post-meta-categories" href="/categories/AI/">AI</a></span><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI-pytorch/"><span class="tags-name tags-punctuation"><i class="solitude fas fa-hashtag"></i>AI pytorch</span></a></div></div></div></div><h1 class="post-title">tansformer 从入门到上手只需要看这一篇</h1><div id="post-meta"><div class="meta-secondline"><span class="post-meta-date" title="Posted on 2025-03-03 00:38:09"><i class="post-meta-icon solitude fas fa-calendar-days"></i><time datetime="2025-03-02T16:38:09.000Z">2025-03-02T16:38:09.000Z</time></span><span class="post-meta-date" title="Last updated on 2025-03-07 00:02:14"><i class="post-meta-icon solitude fas fa-arrows-rotate"></i><time datetime="2025-03-06T16:02:14.113Z">2025-03-06T16:02:14.113Z</time></span><span class="post-meta-position" title="The author's IP belongs toChina"><i class="post-meta-icon solitude fas fa-location-dot"></i><span>China</span></span><a class="post-meta-pv" href="/2025/03/03/transformer1/" title="PV"><i class="post-meta-icon solitude fas fa-fire-flame-curved"></i><span class="waline-pageview-count"><i class="solitude fas fa-spinner fa-spin"></i></span></a></div></div></div><div id="post-info-bottom"></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content article-container"><h1>tansformer 从入门到上手只需要看这一篇</h1>
<h2 id="tansformer-架构解析">tansformer 架构解析</h2>
<h3 id="架构总览">架构总览</h3>
<p><img src= "/img/loading.gif" data-lazy-src="/2025/03/03/transformer1/image.png" alt="Alt text"></p>
<h3 id="词嵌入层-Embeddings">词嵌入层 Embeddings</h3>
<p>输入 token 的索引处在维度为词表大小 d_vocab 的空间里，每个 token 要经过词嵌入层变成维度为 d_model 的特征向量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_vocab</span>):</span><br><span class="line">        <span class="built_in">super</span>(Embeddings, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.lut(x) * math.sqrt(<span class="variable language_">self</span>.d_model)</span><br></pre></td></tr></table></figure>
<p>在原论文，编码器输入、解码器输入以及解码器输出是使用相同参数的 Embeddings，但是在有些地方，发现解码器输出是使用单独的生成器 Generator 结构，原因是因为解码器输出在功能上是获取一个从特征向量到词表空间的概率映射。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Define standard linear + softmax generation step.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_vocab</span>):</span><br><span class="line">        <span class="built_in">super</span>(Generator, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.log_softmax(<span class="variable language_">self</span>.proj(x), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="位置编码-Positional-Encoding">位置编码 Positional Encoding</h3>
<p>为了让模型利用序列的次序，必须注入一些关于序列中的相对位置或绝对位置的信息。为此，在 encoder stack 和 decoder stack 底部的 input embedding 中添加了 positional encoding 。positional encoding 与 embedding 具有相同的维度 ，因此可以将二者相加。</p>
<p>P_j=(PE_{(j,1)}, PE_{(j,2)}...PE_{(j,d_{model})}) \\
PE_{(j,2i)}=\sin(\frac{j}{10000^{2i/d_{model}}}), PE_{(j,2i+1)}=\cos(\frac{j}{10000^{2i/d_{model}}})
</p>
<p>其中,j 表示 position，i 表示维度。即 positional encoding 的每个维度对应于一个正弦曲线，正弦曲线的波长从 2\pi （当维度 i=0 时）到 10000 \times 2\pi （当维度 2i=d_{model} 时）。</p>
<p>对于任意固定的偏移量 k，P_{j+k} 可以表示为 P_j 的线性函数，且正弦版本的 positional embedding 可以应用到训练期间 unseen 的位置，从而可以让模型推断出比训练期间遇到的序列长度更长的序列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;实现Positional Encoding功能&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        位置编码器的初始化函数</span></span><br><span class="line"><span class="string">        :param d_model: 词向量的维度，与输入序列的特征维度相同，512</span></span><br><span class="line"><span class="string">        :param dropout: 置零比率</span></span><br><span class="line"><span class="string">        :param max_len: 句子最大长度,5000</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 如果d_model不是偶数那么后面pe[:, 1::2]会少一个维度，赋值报错。</span></span><br><span class="line">        <span class="keyword">assert</span> d_model % <span class="number">2</span> == <span class="number">0</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 初始化一个nn.Dropout层，设置给定的dropout比例</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化一个位置编码矩阵</span></span><br><span class="line">        <span class="comment"># (5000,512)矩阵，保持每个位置的位置编码，一共5000个位置，每个位置用一个512维度向量来表示其位置编码</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        <span class="comment"># 偶数和奇数在公式上有一个共同部分，使用log函数把次方拿下来，方便计算</span></span><br><span class="line">        <span class="comment"># position表示的是字词在句子中的索引，如max_len是128，那么索引就是从0，1，2，...,127</span></span><br><span class="line">        <span class="comment"># 论文中d_model是512，2i符号中i从0取到255，那么2i对应取值就是0,2,4...510</span></span><br><span class="line">        <span class="comment"># (5000) -&gt; (5000,1)</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 计算用于控制正余弦的系数，确保不同频率成分在d_model维空间内均匀分布</span></span><br><span class="line">        div_term = torch.exp(</span><br><span class="line">            torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 根据位置和div_term计算正弦和余弦值，分别赋值给pe的偶数列和奇数列</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(</span><br><span class="line">            position * div_term</span><br><span class="line">        )  <span class="comment"># 从0开始到最后面，补长为2，其实代表的就是偶数位置</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(</span><br><span class="line">            position * div_term</span><br><span class="line">        )  <span class="comment"># 从1开始到最后面，补长为2，其实代表的就是奇数位置</span></span><br><span class="line">        <span class="comment"># 上面代码获取之后得到的pe:[max_len * d_model]</span></span><br><span class="line">        <span class="comment"># 下面这个代码之后得到的pe形状是：[1 * max_len * d_model]</span></span><br><span class="line">        <span class="comment"># 多增加1维，是为了适应batch_size</span></span><br><span class="line">        <span class="comment"># (5000, 512) -&gt; (1, 5000, 512)</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 将计算好的位置编码矩阵注册为模块缓冲区（buffer），这意味着它将成为模块的一部分并随模型保存与加载，但不会被视为模型参数参与反向传播</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&quot;pe&quot;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):transformer1</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x: [seq_len, batch_size, d_model]  经过词向量的输入</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        x = (</span><br><span class="line">            x + <span class="variable language_">self</span>.pe[:, : x.size(<span class="number">1</span>)].clone().detach()</span><br><span class="line">        )  <span class="comment"># 经过词向量的输入与位置编码相加</span></span><br><span class="line">        <span class="comment"># clone().detach() 组合起来的主要作用是创建一个与原张量数据相同，但不参与梯度计算且内存独立的新张量</span></span><br><span class="line">        <span class="comment"># Dropout层会按照设定的比例随机“丢弃”（置零）一部分位置编码与词向量相加后的元素，</span></span><br><span class="line">        <span class="comment"># 以此引入正则化效果，防止模型过拟合</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.dropout(x)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="多头注意力">多头注意力</h3>
<p><img src= "/img/loading.gif" data-lazy-src="/2025/03/03/transformer1/image-1.png" alt="Alt text"></p>
<h4 id="注意力机制-Scaled-Dot-Product-Attention">注意力机制 Scaled Dot-Product Attention</h4>
<p>原论文叫 Scaled Dot-Product Attention （放缩点积注意力）,架构如上图左侧所示。</p>
<p>注意力机制包括 Query ( Q )，Key ( K )和 Value ( V )三个组成部分，这三个部分由W_Q,W_K,W_V三个线性层生成，这部分原论文架构图没有给出，是我自己补充的，公式如下：</p>
<p>Q(len\_q, d\_k) = X_Q(len\_q, d_model) \cdot W_Q(d_model, d\_k) \\
K(len\_v, d\_k) = X_V(len\_v, d_model) \cdot W_K(d_model, d\_k) \\
V(len\_v, d\_v) = X_V(len\_v, d_model) \cdot W_V(d_model, d\_v)
</p>
<p>其中 Q 和 K 有相同的特征维度 d_k，而 K 和 V 有相同的长度维度 len_v，它们由X_Q,X_V两个输入生成，当X_Q=X_V时计算的是自注意力，当它们不同时计算的就是交叉注意力。</p>
<p>在很多代码中W_Q,W_K,W_V三个线性层是默认有偏置项的，这里为了便于理解没有写出来。</p>
<p>注意力计算公式为：</p>
<p>Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V \\
Q \in \R ^{len_q \times d_k},K \in \R ^{len_v \times d_k},v \in \R ^{len_v \times d_v}, QK^T \in \R ^{len_q \times len_v}, Attention(Q,K,V) \in \R ^{len_q \times d_v}
</p>
<p>内积注意力使用了 1/\sqrt{d_k} 的缩放因子,因为维度越大，则内积中累加和的项越多，内积结果越大。很大的数值会导致 softmax 函数位于饱和区间，梯度几乎为零。</p>
<p>因为输出特征维度为 d_v，所以为了添加残差，通常要求 d_v=d_model 或者也可以引入一个d_v \to d_model的线性层。</p>
<h4 id="多头注意力-Multi-Head-Attention">多头注意力 Multi-Head Attention</h4>
<p>多头注意力将 query, key, value 线性投影 h 次，其中每次线性投影都是不同的并且将 query, key, value 分别投影到 d_k,d_k,d_v 维。然后，在每个 query, key, value 的投影后的版本上，并行执行注意力函数，每个注意力函数产生 d_v 维的 output 。这些 output 被拼接起来并再次投影，产生 final output ，如上图右侧所示。</p>
<p>原论文说，多头注意力允许模型在每个位置联合地关注来自不同子空间的信息。如果只有单个注意力头，那么平均操作会抑制这一点。</p>
<p>MultiHead(Q,K,V)=Concat(head_1,head_2 ... head_h)W^O \\
head_i=Attention(QW^Q_i,KW^K_i,VW^V_i) \\
W^Q_i \in \R ^{d_{model} \times d_k},W^K_i \in \R ^{d_{model} \times d_k},W^V_i \in \R ^{d_{model} \times d_v},W^O \in \R ^{(hd_v) \times d_{model}}
</p>
<p>公式中给出的是更一般的形式,实际上这里的输入 Q 对应上文注意力机制中的X_Q，输入 K 和 V 对应上文注意力机制中的X_V，另外原论文中 d*model=512， d_k=d_v=d*{model}/h=64。</p>
<p>在 Transformer 中以三种不同的方式使用多头注意力：</p>
<ol>
<li>在 encoder-decoder attention 层中，query 来自于前一个 decoder layer，key 和 value 来自于 encoder 的输出。这允许 decoder 中的每个位置关注 input 序列中的所有位置。这模仿了 sequence-to-sequence 模型中典型的 encoder-decoder attention 注意力机制。</li>
<li>encoder 包含自注意力层。在自注意力层中，所有的 query, key, value 都来自于同一个地方（在这个 case 中，就是 encoder 中前一层的输出）。encoder 中的每个位置都可以关注 encoder 上一层中的所有位置。</li>
<li>类似地，decoder 中的自注意力层允许 decoder 中的每个位置关注 decoder 中截至到当前为止（包含当前位置）的所有位置。我们需要防止 decoder 中的信息向左流动，从而保持自回归特性。我们通过在 scaled dot-product attention 内部屏蔽掉 softmax input 的某些 value 来实现这一点（将这些 value 设置为 -1e9,一个很小的数），这些 value 对应于无效连接 illegal connection。</li>
</ol>
<h4 id="代码实现-pytorch">代码实现 pytorch</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="string">&quot;Produce N identical layers.&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    query: (len_q, d_k)</span></span><br><span class="line"><span class="string">    key: (len_v, d_k)</span></span><br><span class="line"><span class="string">    value: (len_v, d_v)</span></span><br><span class="line"><span class="string">    mask: (len_q, len_v) fill true</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>))/math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask, -<span class="number">1e9</span>)</span><br><span class="line">    p_attn = torch.softmax(scores, dim = -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Take in model size and number of heads.</span></span><br><span class="line"><span class="string">        We assume d_v always equals d_k.</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.d_k = d_model // h</span><br><span class="line">        <span class="variable language_">self</span>.h = h</span><br><span class="line">        <span class="variable language_">self</span>.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        <span class="variable language_">self</span>.attn = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        query: (len_q, d_k)</span></span><br><span class="line"><span class="string">        key: (len_v, d_k)</span></span><br><span class="line"><span class="string">        value: (len_v, d_k)</span></span><br><span class="line"><span class="string">        mask: (len_q, len_v) fill true</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span></span><br><span class="line">        query, key, value = [</span><br><span class="line">            lin(x).view(nbatches, -<span class="number">1</span>, <span class="variable language_">self</span>.h, <span class="variable language_">self</span>.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">for</span> lin, x <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.linears, (query, key, value))</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch.</span></span><br><span class="line">        x, <span class="variable language_">self</span>.attn = attention(</span><br><span class="line">            query, key, value, mask=mask, dropout=<span class="variable language_">self</span>.dropout</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear.</span></span><br><span class="line">        x = (</span><br><span class="line">            x.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">            .contiguous()</span><br><span class="line">            .view(nbatches, -<span class="number">1</span>, <span class="variable language_">self</span>.h * <span class="variable language_">self</span>.d_k)</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">del</span> query</span><br><span class="line">        <span class="keyword">del</span> key</span><br><span class="line">        <span class="keyword">del</span> value</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linears[-<span class="number">1</span>](x)</span><br></pre></td></tr></table></figure>
<h3 id="编码器掩码-length-mask">编码器掩码 length mask</h3>
<p><img src= "/img/loading.gif" data-lazy-src="/2025/03/03/transformer1/image-3.png" alt="Alt text"><br>
对于输入 x，其原始序列长度是长短不一的，但是为了放入张量中进行训练，会对短序列填补到最大长度，使输入序列的长度统一。这样在计算注意力时，填补的维度应该是无效的不能参与计算。</p>
<p>注意力计算中 socre 的维度为(len_q, len_v),因为 torch.softmax(scores, dim = -1)计算最后一个维度列，对不同列之间进行 softmax，所以实际上是对每行数据 softmax。这样要保证每行不能全被 mask，防止计算出现 NaN。这样对 len_v 列维度进行 mask 就行了，len_q 对应部分最终计算 loss 时会被过滤掉，没有影响。</p>
<p>这里预期的输入 batch_mask (N, len) 是已经处理好的输入序列的掩码（True 代码填充部分），只需要在批次维度N和序列长度维度len之间填充一个维度，那么在 scores.masked_fill(mask, -1e9) 时，最后维度 len_v 的掩码会自动在前面维度 len_q 上广播，而不用考虑 len_q 的长度变化，这样这个掩码既可以在编码器计算自注意力时用到，又可以用在计算交叉注意力时。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">len_mask</span>(<span class="params">batch_mask</span>) -&gt; torch.Tensor:</span><br><span class="line">    <span class="keyword">return</span> batch_mask.clone().unsqueeze(-<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里给出了第二中把 len_v 维度的掩码广播到新增的 len_q 维度的方法。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">len_mask2</span>(<span class="params">batch_mask, len_q</span>) -&gt; torch.Tensor:</span><br><span class="line">    <span class="keyword">return</span> batch_mask.clone().unsqueeze(-<span class="number">2</span>).expand(*batch_mask.shape, len_q).transpose(-<span class="number">1</span>, -<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h3 id="解码器掩码-causal-mask">解码器掩码 causal mask</h3>
<p>我们给模型的完整输入为 (x_1, x_2...x_n) ，但是预测 x_i 时只用到了前 i-1 个输入的信息，也就是说后面的信息我们应当 mask 掉，只能根据历史信息来预测当前位置的输出。这很好理解，假如模型的输入为“早上好”，那么应该通过“早”来预测“上”，通过“早上”来预测“好”，而不是通过“早上好”来预测“上”或者“好”，因为这相当于直接把 ground truth 告诉模型了。</p>
<p><img src= "/img/loading.gif" data-lazy-src="/2025/03/03/transformer1/image-2.png" alt="Alt text"><br>
在解码器的自注意力层，socre 的维度为 (len_q, len_v)，我们只需要如上图生成一个上三角矩阵，将对角线以上的部分 mask 就行了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">subsequent_mask</span>(<span class="params">size</span>):</span><br><span class="line">    <span class="string">&quot;Mask out subsequent positions.&quot;</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=<span class="number">1</span>).<span class="built_in">type</span>(</span><br><span class="line">        torch.uint8</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> subsequent_mask == <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="标准化">标准化</h3>
<p>这里对末尾维度也就是特征维度进行标准化，标准化后还通过了一个线性层。</p>
<p>这里分别对线性层的权重初始化为 1，偏置初始化为 0。而 nn.liner 默认使用 Kaiming 初始化权重，均匀分布初始化偏置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Construct a layernorm module (See citation for details).&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        <span class="variable language_">self</span>.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        <span class="variable language_">self</span>.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.a_2 * (x - mean) / (std + <span class="variable language_">self</span>.eps) + <span class="variable language_">self</span>.b_2</span><br></pre></td></tr></table></figure>
<h3 id="残差连接-residual-connections">残差连接 residual connections</h3>
<p>这里对传入的子层标准化后添加残差。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SublayerConnection</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.norm = LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line">        <span class="string">&quot;Apply residual connection to any sublayer with the same size.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> x + <span class="variable language_">self</span>.dropout(sublayer(<span class="variable language_">self</span>.norm(x)))</span><br></pre></td></tr></table></figure>
<h3 id="逐位置前馈网络-Position-wise-Feed-Forward-Networks">逐位置前馈网络 Position-wise Feed-Forward Networks</h3>
<p>FFN(x)=max(0, xW_1+b_1)W_2+b_2
</p>
<p>先将 x 从 d_model=512 维投影到 dff=2048 维，再投影回 d_model=512 维。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Implements FFN equation.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        <span class="variable language_">self</span>.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.w_2(<span class="variable language_">self</span>.dropout(<span class="variable language_">self</span>.w_1(x).relu()))</span><br></pre></td></tr></table></figure>
<h3 id="编码器-Ecoder">编码器 Ecoder</h3>
<p>对于单个编码器，先通过残差连接包裹的 self_attn 自注意力层，再通过残差连接包裹的 feed_forward 逐位置前馈网络层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Encoder is made up of self-attn and feed forward (defined below)&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, self_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.self_attn = self_attn</span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = feed_forward</span><br><span class="line">        <span class="variable language_">self</span>.sublayer = clones(SublayerConnection(d_model, dropout), <span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.size = d_model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (left) for connections.&quot;</span></span><br><span class="line">        x = <span class="variable language_">self</span>.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: <span class="variable language_">self</span>.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.sublayer[<span class="number">1</span>](x, <span class="variable language_">self</span>.feed_forward)</span><br></pre></td></tr></table></figure>
<p>完整编码器就是把上面的编码重复 N 层，然后通过一个标准化层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Core encoder is a stack of N layers&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.layers = clones(layer, N)</span><br><span class="line">        <span class="variable language_">self</span>.norm = LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;Pass the input (and mask) through each layer in turn.&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.norm(x)</span><br></pre></td></tr></table></figure>
<h3 id="解码器-Decoder">解码器 Decoder</h3>
<p>单个解码器里有两个注意力层，第一个注意力层先对输出结果计算自注意力，再将结果输出到第二个注意力层与编码器的输出结果一起计算交叉注意力，随后进入一个逐位置前馈网络层，这三层都使用前面定义的残差连接包裹。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Decoder is made of self-attn, src-attn, and feed forward (defined below)&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, src_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.size = size</span><br><span class="line">        <span class="variable language_">self</span>.self_attn = self_attn</span><br><span class="line">        <span class="variable language_">self</span>.src_attn = src_attn</span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = feed_forward</span><br><span class="line">        <span class="variable language_">self</span>.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (right) for connections.&quot;</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = <span class="variable language_">self</span>.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: <span class="variable language_">self</span>.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = <span class="variable language_">self</span>.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: <span class="variable language_">self</span>.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.sublayer[<span class="number">2</span>](x, <span class="variable language_">self</span>.feed_forward)</span><br></pre></td></tr></table></figure>
<p>完整解码器就是把上面的解码层重复 N 层，然后通过一个标准化层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Generic N layer decoder with masking.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.layers = clones(layer, N)</span><br><span class="line">        <span class="variable language_">self</span>.norm = LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.norm(x)</span><br></pre></td></tr></table></figure>
<h3 id="Transformer">Transformer</h3>
<p><img src= "/img/loading.gif" data-lazy-src="/2025/03/03/transformer1/image.png" alt="Alt text"><br>
把以上组件组合到一起构成 Transformer 架构，其中编码器和解码器的掩码生成代码没有加进去，因为掩码是要随数据一起初始化的，并不和网络结构一起初始化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A standard Transformer architecture.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, src_vocab, tgt_vocab, N=<span class="number">6</span>, d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        c = copy.deepcopy</span><br><span class="line">        attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">        ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">        position = PositionalEncoding(d_model, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.encoder = Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)</span><br><span class="line">        <span class="variable language_">self</span>.decoder = Decoder(</span><br><span class="line">            DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.src_embed = nn.Sequential(Embeddings(d_model, src_vocab), c(position))</span><br><span class="line">        <span class="variable language_">self</span>.tgt_embed = nn.Sequential(Embeddings(d_model, tgt_vocab), c(position))</span><br><span class="line">        <span class="variable language_">self</span>.generator = Generator(d_model, tgt_vocab)</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> <span class="variable language_">self</span>.parameters():</span><br><span class="line">            <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">                nn.init.xavier_uniform_(p)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, tgt, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;Take in and process masked src and target sequences.&quot;</span></span><br><span class="line">        out = <span class="variable language_">self</span>.decode(<span class="variable language_">self</span>.encode(src, src_mask), src_mask, tgt, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.generator(out)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, src, src_mask</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.encoder(<span class="variable language_">self</span>.src_embed(src), src_mask)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, memory, src_mask, tgt, tgt_mask</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.decoder(<span class="variable language_">self</span>.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure>
<h3 id="测试">测试</h3>
<p>测试一下是否能跑通，测试没有问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">inference_test</span>():</span><br><span class="line">    test_model = Transformer(<span class="number">11</span>, <span class="number">11</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># test_model.eval() 评估模式，比如 Dropout 层在评估和训练时表现不同。</span></span><br><span class="line">    test_model.<span class="built_in">eval</span>()</span><br><span class="line">    src = torch.LongTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]])</span><br><span class="line">    src_mask = torch.ones(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">    src_mask = src_mask == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    memory = test_model.encode(src, src_mask)</span><br><span class="line">    ys = torch.zeros(<span class="number">1</span>, <span class="number">1</span>).type_as(src)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">9</span>):</span><br><span class="line">        out = test_model.decode(memory, src_mask, ys, subsequent_mask(ys.size(<span class="number">1</span>)))</span><br><span class="line">        prob = test_model.generator(out[:, -<span class="number">1</span>])</span><br><span class="line">        _, next_word = torch.<span class="built_in">max</span>(prob, dim=<span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat(</span><br><span class="line">            [ys, torch.empty(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Example Untrained Model Prediction:&quot;</span>, ys)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_tests</span>():</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        inference_test()</span><br></pre></td></tr></table></figure>
<h2 id="训练第一个模型">训练第一个模型</h2>
<p>这里我在网上找到了阿里云的天池数据集打榜挑战赛，并选择了中文医疗信息处理评测基准 CBLUE 中的中文医学命名实体识别 V2（CMeEE-V2）任务。</p>
<h3 id="下载数据">下载数据</h3>
<p>数据好像下载要申请，挺麻烦，我直接在 huggingface 上找到了相同数据集。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 先配置 git lfs ,不然大文件不会克隆下来</span><br><span class="line">conda install git-lfs</span><br><span class="line">git lfs install</span><br><span class="line"># 对于大于5G的huggingface数据，还需要</span><br><span class="line">pip install huggingface_hub</span><br><span class="line">huggingface-cli lfs-enable-largefiles .</span><br><span class="line"># 克隆数据</span><br><span class="line">git clone https://huggingface.co/datasets/Rosenberg/CMeEE-V2</span><br></pre></td></tr></table></figure>
<h3 id="训练">训练</h3>
<p>将之前的模型部分的代码整理到 <a target="_blank" rel="noopener" href="http://transformer.py">transformer.py</a> 文件中，然后导入里面的类和函数。<br>
测试通过，可以正常训练。</p>
<p>我设置 N=2, d_model=128, d_ff=512, batch_size=5 ，已经挺小的了，训练一个但是还是需要4个G的显存，以及每训练一个epoch需要2.9小时的时间。时间和本地电脑资源实在不太够用。</p>
<p>训练结果和代码已上传git：<br>
<a target="_blank" rel="noopener" href="https://github.com/hs3434/CMeEE-V2">https://github.com/hs3434/CMeEE-V2</a></p>
<p>从loss来看，是随着训练下降的。<br>
<img src= "/img/loading.gif" data-lazy-src="/2025/03/03/transformer1/loss.svg" alt="Alt text"><br>
<img src= "/img/loading.gif" data-lazy-src="/2025/03/03/transformer1/Average_Loss.svg" alt="Alt text"></p>
</article><div class="post-copyright"><div class="post-copyright__author_group"><div class="post-copyright__author_img"><img class="post-copyright__author_img_front" src= "/img/loading.gif" data-lazy-src="/img/logo.png"></div><div class="post-copyright__author_name">Shane Hu</div><div class="post-copyright__author_desc">Welcome, friends!</div></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div id="quit-box" onclick="RemoveRewardMask()"></div></div></div><div class="social-share"><a class="social-share-ico icon-qq" target="_blank" rel="noopener" href="https://connect.qq.com/widget/shareqq/index.html?url=https%3A%2F%2Fhs3434.github.io%2Fhs3434.github.io%2F2025%2F03%2F03%2Ftransformer1%2F&amp;title=tansformer%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E4%B8%8A%E6%89%8B%E5%8F%AA%E9%9C%80%E8%A6%81%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87&amp;desc=undefined&amp;summary=undefined&amp;site=tansformer%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E4%B8%8A%E6%89%8B%E5%8F%AA%E9%9C%80%E8%A6%81%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87&amp;pics=2025%2F03%2F03%2Ftransformer1%2Fimage.png" title="Share to QQ"><i class="solitude fab fa-qq"></i></a><a class="social-share-ico icon-weibo" target="_blank" rel="noopener" href="http://service.weibo.com/share/share.php?url=https%3A%2F%2Fhs3434.github.io%2Fhs3434.github.io%2F2025%2F03%2F03%2Ftransformer1%2F&amp;title=tansformer%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E4%B8%8A%E6%89%8B%E5%8F%AA%E9%9C%80%E8%A6%81%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87&amp;pic=2025%2F03%2F03%2Ftransformer1%2Fimage.png" title="Share to Weibo"><i class="solitude fab fa-weibo"></i></a><a class="social-share-ico icon-twitter" target="_blank" rel="noopener" href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fhs3434.github.io%2Fhs3434.github.io%2F2025%2F03%2F03%2Ftransformer1%2F&amp;text=tansformer%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E4%B8%8A%E6%89%8B%E5%8F%AA%E9%9C%80%E8%A6%81%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87" title="Share to Twitter"><i class="solitude fab fa-twitter"></i></a><a class="social-share-ico icon-facebook" target="_blank" rel="noopener" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fhs3434.github.io%2Fhs3434.github.io%2F2025%2F03%2F03%2Ftransformer1%2F" title="Share to Facebook"><i class="solitude fab fa-facebook"></i></a><a class="social-share-ico icon-telegram" target="_blank" rel="noopener" href="https://t.me/share/url?url=https%3A%2F%2Fhs3434.github.io%2Fhs3434.github.io%2F2025%2F03%2F03%2Ftransformer1%2F&amp;text=tansformer%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E4%B8%8A%E6%89%8B%E5%8F%AA%E9%9C%80%E8%A6%81%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87" title="Share to Telegram"><i class="solitude fab fa-telegram"></i></a><a class="social-share-ico icon-whatsapp" target="_blank" rel="noopener" href="https://api.whatsapp.com/send?text=tansformer%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E4%B8%8A%E6%89%8B%E5%8F%AA%E9%9C%80%E8%A6%81%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87 https%3A%2F%2Fhs3434.github.io%2Fhs3434.github.io%2F2025%2F03%2F03%2Ftransformer1%2F" title="Share to WhatsApp"><i class="solitude fab fa-whatsapp"></i></a><a class="social-share-ico icon-linkedin" target="_blank" rel="noopener" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3A%2F%2Fhs3434.github.io%2Fhs3434.github.io%2F2025%2F03%2F03%2Ftransformer1%2F&amp;title=tansformer%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E4%B8%8A%E6%89%8B%E5%8F%AA%E9%9C%80%E8%A6%81%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87&amp;summary=undefined&amp;source=tansformer%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E4%B8%8A%E6%89%8B%E5%8F%AA%E9%9C%80%E8%A6%81%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87" title="Share to LinkedIn"><i class="solitude fab fa-linkedin"></i></a><div class="social-share-ico icon-link" onclick="utils.copy(&quot;https://hs3434.github.io/hs3434.github.io/2025/03/03/transformer1/&quot;)" title="Share to Link"><i class="solitude fas fa-link"></i></div><div class="social-share-ico icon-qrcode" title="Share to QR code"><i class="solitude fas fa-qrcode"></i><div class="share-main"><div class="share-main-all"><div id="qrcode"></div><div class="reward-dec">Share to QR code</div></div></div><script pjax>typeof QRCode === 'function' && new QRCode(document.getElementById("qrcode"), {
    text: 'https://hs3434.github.io/hs3434.github.io/2025/03/03/transformer1/',
    correctLevel : QRCode.CorrectLevel.L
});
window.addEventListener('DOMContentLoaded', () => {
    new QRCode(document.getElementById("qrcode"), {
        text: 'https://hs3434.github.io/hs3434.github.io/2025/03/03/transformer1/',
        correctLevel : QRCode.CorrectLevel.L
    });
});
</script></div></div><div class="post-copyright__notice"><span class="post-copyright-info">This piece of writing is an original article, utilizing the<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-hans">CC BY-NC-SA 4.0</a>Agreement. For complete reproduction, please acknowledge the source as Courtesy of<a href="/">Blog of Shane Hu</a></span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI-pytorch/"><span class="tags-punctuation"><i class="solitude fas fa-hashtag"></i>AI pytorch<span class="tagsPageCount">1</span></span></a></div></div></div><nav class="needEndHide pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/03/06/250306/"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">vscode 打开powershell出现编码报错问题解决</div></div></a></div><div class="next-post pull-right"><a href="/2024/06/20/2406202/"><div class="pagination-info"><div class="label">Next</div><div class="next_info">使用 dnscrypt-proxy 解决 dns 被劫持污染问题</div></div></a></div></nav><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="solitude fas fa-comment"></i><span>  Comment</span><span class="count"> (<span class="waline-comment-count"><i class="solitude fas fa-spinner fa-spin"></i></span>)</span></div></div><div class="comment-wrap"><div id="waline-wrap"></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><div class="top-group"><div class="sayhi" id="sayhi" onclick="sco.changeWittyWord()"></div></div></div><div class="avatar"><img alt="Avatar" src= "/img/loading.gif" data-lazy-src="/img/head_porttrait.jpeg"></div><div class="description"></div><div class="bottom-group"><span class="left"><div class="name">Shane Hu</div><div class="desc">长风破浪会有时，直挂云帆济沧海。</div></span><div class="social-icons is-center"><a class="social-icon" target="_blank" rel="noopener" href="https://github.com/hs3434" title="Github"><i class="solitude  st-github-line "></i></a></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="solitude fas fa-bars"></i><span>Table of contents</span></div><div class="toc-content" id="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">tansformer 从入门到上手只需要看这一篇</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#tansformer-%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90"><span class="toc-text">tansformer 架构解析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84%E6%80%BB%E8%A7%88"><span class="toc-text">架构总览</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E5%B5%8C%E5%85%A5%E5%B1%82-Embeddings"><span class="toc-text">词嵌入层 Embeddings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-Positional-Encoding"><span class="toc-text">位置编码 Positional Encoding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-text">多头注意力</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-Scaled-Dot-Product-Attention"><span class="toc-text">注意力机制 Scaled Dot-Product Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B-Multi-Head-Attention"><span class="toc-text">多头注意力 Multi-Head Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-pytorch"><span class="toc-text">代码实现 pytorch</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8%E6%8E%A9%E7%A0%81-length-mask"><span class="toc-text">编码器掩码 length mask</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8%E6%8E%A9%E7%A0%81-causal-mask"><span class="toc-text">解码器掩码 causal mask</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-text">标准化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5-residual-connections"><span class="toc-text">残差连接 residual connections</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%90%E4%BD%8D%E7%BD%AE%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C-Position-wise-Feed-Forward-Networks"><span class="toc-text">逐位置前馈网络 Position-wise Feed-Forward Networks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-Ecoder"><span class="toc-text">编码器 Ecoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8-Decoder"><span class="toc-text">解码器 Decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer"><span class="toc-text">Transformer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95"><span class="toc-text">测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9E%8B"><span class="toc-text">训练第一个模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-text">下载数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-text">训练</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="solitude fas fa-map"></i><span>New posts</span></div><div class="aside-list"><a class="aside-list-item" href="/2025/03/06/250306/" title="vscode 打开powershell出现编码报错问题解决"><div class="thumbnail"><img alt="vscode 打开powershell出现编码报错问题解决" src= "/img/loading.gif" data-lazy-src="/img/default.png"></div><div class="content"><span class="title" href="/2025/03/06/250306/" title="vscode 打开powershell出现编码报错问题解决">vscode 打开powershell出现编码报错问题解决</span><span class="categories" href="/2025/03/06/250306/">vscode</span></div></a><a class="aside-list-item" href="/2025/03/03/transformer1/" title="tansformer 从入门到上手只需要看这一篇"><div class="thumbnail"><img alt="tansformer 从入门到上手只需要看这一篇" src= "/img/loading.gif" data-lazy-src="/2025/03/03/transformer1/image.png"></div><div class="content"><span class="title" href="/2025/03/03/transformer1/" title="tansformer 从入门到上手只需要看这一篇">tansformer 从入门到上手只需要看这一篇</span><span class="categories" href="/2025/03/03/transformer1/">AI</span></div></a><a class="aside-list-item" href="/2024/06/20/2406202/" title="使用 dnscrypt-proxy 解决 dns 被劫持污染问题"><div class="thumbnail"><img alt="使用 dnscrypt-proxy 解决 dns 被劫持污染问题" src= "/img/loading.gif" data-lazy-src="/img/default.png"></div><div class="content"><span class="title" href="/2024/06/20/2406202/" title="使用 dnscrypt-proxy 解决 dns 被劫持污染问题">使用 dnscrypt-proxy 解决 dns 被劫持污染问题</span><span class="categories" href="/2024/06/20/2406202/">jottings</span></div></a><a class="aside-list-item" href="/2024/06/20/240620/" title="debian 使用 dnscrypt-proxy 后 53 端口被  1/init systemd 占用问题"><div class="thumbnail"><img alt="debian 使用 dnscrypt-proxy 后 53 端口被  1/init systemd 占用问题" src= "/img/loading.gif" data-lazy-src="/img/default.png"></div><div class="content"><span class="title" href="/2024/06/20/240620/" title="debian 使用 dnscrypt-proxy 后 53 端口被  1/init systemd 占用问题">debian 使用 dnscrypt-proxy 后 53 端口被  1/init systemd 占用问题</span><span class="categories" href="/2024/06/20/240620/">jottings</span></div></a><a class="aside-list-item" href="/2024/06/19/240619/" title="HTTP Status 307缓存的处理"><div class="thumbnail"><img alt="HTTP Status 307缓存的处理" src= "/img/loading.gif" data-lazy-src="/img/default.png"></div><div class="content"><span class="title" href="/2024/06/19/240619/" title="HTTP Status 307缓存的处理">HTTP Status 307缓存的处理</span><span class="categories" href="/2024/06/19/240619/">jottings</span></div></a></div></div></div></div></main><footer id="footer"><div id="st-footer-bar"><div class="footer-logo"><span>主页</span></div><div class="footer-bar-description">来自 Shane Hu 的文章</div><a class="footer-bar-link" href="/about/">了解更多</a></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div class="copyright">© 2024 - 2025 By&nbsp;<a class="footer-bar-link" href="/"><img class="author-avatar" src= "/img/loading.gif" data-lazy-src="/img/pwa/favicon.png">Shane Hu</a></div><div class="beian-group"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://hexo.io/">Framework: Hexo</a><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/everfu/hexo-theme-solitude">Theme: Solitude</a></div></div></div></div><div class="comment-barrage needEndHide"></div></footer></div><!-- right_menu--><!-- inject body--><div><script src="/js/utils.js?v=3.0.14"></script><script src="/js/main.js?v=3.0.14"></script><script src="/js/third_party/waterfall.min.js?v=3.0.14"></script><script src="https://fastly.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="https://fastly.jsdelivr.net/npm/qrcodejs@1.0.0/qrcode.min.js"></script><script src="https://fastly.jsdelivr.net/npm/typeit@8.8.7/dist/index.umd.min.js"></script><script src="/js/third_party/universe.min.js?v=3.0.14"></script><script>dark()
</script><script src="/js/tw_cn.js?v=3.0.14"></script><script src="https://fastly.jsdelivr.net/npm/katex@0.16.21/dist/contrib/copy-tex.min.js"><script>(() => {
  document.querySelectorAll('.article-container span.katex-display').forEach(item => {
    utils.wrap(item, 'div', {class: 'katex-wrap'})
  })
})();
</script></script><script src="https://fastly.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><script src="https://fastly.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.js"></script><script>window.paceOptions = {
  restartOnPushState: false
}

utils.addGlobalFn('pjaxSend', () => {
  Pace.restart()
}, 'pace_restart')
</script><script src="https://fastly.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><div class="js-pjax"><script>(() => {
    let walineInitFunction = window.walineFn || null

    function initWaline(initFn) {
        const walineOptions = {
            el: '#waline-wrap',
            serverURL: '',
            pageview: false,
            dark: 'html[data-theme="dark"]',
            path: window.location.pathname,
            comment: true,
            ...null
        }
        const walineInstance = initFn(walineOptions)
        utils.addGlobalFn('pjax', () => walineInstance.destroy(), 'destroyWaline')
        GLOBAL_CONFIG.lightbox && utils.lightbox(document.querySelectorAll('#comment .wl-content img:not(.wl-emoji)'))
        sco.owoBig({
            body: '.wl-emoji-popup',
            item: '.wl-tab-wrapper button'
        })
    }

    async function loadWaline() {
        if (walineInitFunction) initWaline(walineInitFunction)
        else {
            await utils.getCSS('https://fastly.jsdelivr.net/npm/@waline/client@3.4.3/dist/waline.min.css')
            const {init} = await import('https://fastly.jsdelivr.net/npm/@waline/client@3.4.3/dist/waline.min.js')
            walineInitFunction = init || Waline.init
            initWaline(walineInitFunction)
            window.walineFn = walineInitFunction
        }
        true && barrageWaline()
    }

    if (true || true) {
        if (true) utils.loadComment(document.getElementById('waline-wrap'), loadWaline)
        else loadWaline()
    } else window.loadTwoComment = loadWaline
})()
</script><script>async function barrageWaline() {
    const url = new URL('/api/comment')
    const params = {path: window.location.pathname, sortBy: 'insertedAt_asc'}
    Object.entries(params).forEach(([key, value]) => url.searchParams.append(key, value))
    await fetch(url).then(async res => {
        if (!res.ok) throw new Error(`HTTP error! status: ${res.status}`)
        const data = await res.json();
        const regex = /<img [^>]*class="wl-emoji"[^>]*>/;
        const init = () => {
            initializeCommentBarrage(data.data.data
                .map(item => ({
                    nick: item.nick,
                    mailId: item.avatar,
                    content: item.comment.replace(regex, ''),
                    id: item.objectId
                })))
        }
        if (typeof initializeCommentBarrage === "undefined") await utils.getScript('/js/third_party/barrage.min.js?v=3.0.14').then(init)
        else init()
    }).catch(error => console.error("An error occurred while fetching comments: ", error))
}</script></div></div><!-- pjax--><script>const pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: ["title","#body-wrap","#site-config","meta[name=\"description\"]",".js-pjax","meta[property^=\"og:\"]","#config-diff",".rs_show",".rs_hide"],
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
})

document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
})

document.addEventListener('pjax:complete', () => {
    window.refreshFn()

    document.querySelectorAll('script[data-pjax]').forEach(item => {
        const newScript = document.createElement('script')
        const content = item.text || item.textContent || item.innerHTML || ""
        Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
        newScript.appendChild(document.createTextNode(content))
        item.parentNode.replaceChild(newScript, item)
    })

    GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

})

document.addEventListener('pjax:error', (e) => {
    if (e.request.status === 404) {
        pjax.loadUrl('/404.html')
    }
})</script><!-- google adsense--><!-- search--><!-- music--></body></html>
        <script>
            const posts = ["2025/03/06/250306/","2025/03/03/transformer1/","2024/06/20/2406202/","2024/06/20/240620/","2024/06/19/240619/","2024/06/06/Dante/","2024/06/06/solitude/","2024/06/05/udesk2/","2024/06/05/udesk1/","2024/06/04/Advanced-Algebra/","2024/06/04/server/","2024/06/04/font/","2024/06/04/ydns/","2024/06/04/conda/","2024/06/04/bind9/","2024/06/04/openwrt/","2024/06/04/ssl/","2024/06/03/hexo/"];
            function toRandomPost() {
                const randomPost = posts[Math.floor(Math.random() * posts.length)];
                pjax.loadUrl(GLOBAL_CONFIG.root + randomPost);
            }
        </script>