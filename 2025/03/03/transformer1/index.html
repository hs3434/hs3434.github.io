<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><title>tansformer ä»å…¥é—¨åˆ°ä¸Šæ‰‹åªéœ€è¦çœ‹è¿™ä¸€ç¯‡ | Blog of Shane Hu</title><noscript>Please enable JavaScript to view the site</noscript><link rel="icon" href="/img/pwa/favicon.png"><!-- index.css--><link rel="stylesheet" href="/css/index.css?v=3.0.14"><!-- inject head--><link rel="canonical" href="https://hs3434.github.io/hs3434.github.io/2025/03/03/transformer1/index.html"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css"><!-- aplayer--><!-- swiper--><!-- fancybox ui--><!-- katex--><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css"><!-- Open Graph--><meta name="description" content="tansformer ä»å…¥é—¨åˆ°ä¸Šæ‰‹åªéœ€è¦çœ‹è¿™ä¸€ç¯‡ tansformer æ¶æ„è§£æ æ¶æ„æ€»è§ˆ è¯åµŒå…¥å±‚ Embeddings è¾“å…¥ token çš„ç´¢å¼•å¤„åœ¨ç»´åº¦ä¸ºè¯è¡¨å¤§å° d_vocab çš„ç©ºé—´é‡Œï¼Œæ¯ä¸ª token è¦ç»è¿‡è¯åµŒå…¥å±‚å˜æˆç»´åº¦ä¸º d_model çš„ç‰¹å¾å‘é‡ã€‚ 12345678class E"><!-- pwa--><meta name="apple-mobile-web-app-capable" content="Blog of Shane Hu"><meta name="theme-color" content="var(--efu-main)"><meta name="apple-mobile-web-app-status-bar-style" content="var(--efu-main)"><link rel="bookmark" href="/img/pwa/favicon.png"><link rel="apple-touch-icon" href="/img/pwa/favicon.png" sizes="180x180"><script>console.log(' %c Solitude %c ' + '3.0.14' + ' %c https://github.com/everfu/hexo-theme-solitude',
    'background:#35495e ; padding: 1px; border-radius: 3px 0 0 3px;  color: #fff',
    'background:#ff9a9a ; padding: 1px; border-radius: 0 3px 3px 0;  color: #fff',
    'background:unset ; padding: 1px; border-radius: 0 3px 3px 0;  color: #fff')
</script><script>let mdate = "7-7,9-18,12-13";
mdate = (mdate.split(","));
let ndate = new Date();
for (let i of mdate) {
    if (i === (ndate.getMonth()+1) + "-" + (ndate.getDate())) {
        document.documentElement.classList.add('memorial');
    }
}
</script><script>(()=>{
        const saveToLocal = {
            set: function setWithExpiry(key, value, ttl) {
                if (ttl === 0)
                    return
                const now = new Date()
                const expiryDay = ttl * 86400000
                const item = {
                    value: value,
                    expiry: now.getTime() + expiryDay
                }
                localStorage.setItem(key, JSON.stringify(item))
            },
            get: function getWithExpiry(key) {
                const itemStr = localStorage.getItem(key)

                if (!itemStr) {
                    return undefined
                }
                const item = JSON.parse(itemStr)
                const now = new Date()

                if (now.getTime() > item.expiry) {
                    localStorage.removeItem(key)
                    return undefined
                }
                return item.value
            }
        };
        window.utils = {
            saveToLocal: saveToLocal,
            getCSS: (url, id = false) => new Promise((resolve, reject) => {
              const link = document.createElement('link')
              link.rel = 'stylesheet'
              link.href = url
              if (id) link.id = id
              link.onerror = reject
              link.onload = link.onreadystatechange = function() {
                const loadState = this.readyState
                if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
                link.onload = link.onreadystatechange = null
                resolve()
              }
              document.head.appendChild(link)
            }),
            getScript: (url, attr = {}) => new Promise((resolve, reject) => {
              const script = document.createElement('script')
              script.src = url
              script.async = true
              script.onerror = reject
              script.onload = script.onreadystatechange = function() {
                const loadState = this.readyState
                if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
                script.onload = script.onreadystatechange = null
                resolve()
              }

              Object.keys(attr).forEach(key => {
                script.setAttribute(key, attr[key])
              })

              document.head.appendChild(script)
            }),
            addGlobalFn: (key, fn, name = false, parent = window) => {
                const globalFn = parent.globalFn || {}
                const keyObj = globalFn[key] || {}

                if (name && keyObj[name]) return

                name = name || Object.keys(keyObj).length
                keyObj[name] = fn
                globalFn[key] = keyObj
                parent.globalFn = globalFn
            },
            addEventListenerPjax: (ele, event, fn, option = false) => {
              ele.addEventListener(event, fn, option)
              utils.addGlobalFn('pjax', () => {
                  ele.removeEventListener(event, fn, option)
              })
          },
        }
    })()</script><!-- theme--><script>initTheme = () => {
    let isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const cachedMode = utils.saveToLocal.get('theme');
    if (cachedMode === undefined) {
        const nowMode =
            isDarkMode ? 'dark' : 'light'
        document.documentElement.setAttribute('data-theme', nowMode);
    } else {
        document.documentElement.setAttribute('data-theme', cachedMode);
    }
    typeof rm === 'object' && rm.mode(cachedMode === 'dark' && isDarkMode)
}
initTheme()</script><!-- global head--><script>const GLOBAL_CONFIG = {
    root: '/',
    algolia: undefined,
    localsearch: undefined,
    runtime: '2024-06-03 00:00:00',
    lazyload: {
        enable: true,
        error: '/img/error_load.avif'
    },
    copyright: false,
    highlight: {"limit":200,"expand":true,"copy":true,"syntax":"highlight.js"},
    randomlink: false,
    lang: {"theme":{"dark":"Dark","light":"Light"},"copy":{"success":"Copied","error":"Copy failed"},"backtop":"Back to top","time":{"day":" days ago","hour":" hours ago","just":"just","min":" minutes ago","month":" months ago"},"day":" days","f12":"Developer mode is turned on, please follow the GPL.","totalk":"You don't need to delete blank lines, just type in your comments.","barrage":{"title":"Hot"}},
    aside: {
        state: {
            morning: "âœ¨ æ—©ä¸Šå¥½ï¼Œæ–°çš„ä¸€å¤©å¼€å§‹äº†",
            noon: "ğŸ² åˆé¤æ—¶é—´",
            afternoon: "ğŸŒ ä¸‹åˆå¥½",
            night: "æ—©ç‚¹ä¼‘æ¯",
            goodnight: "æ™šå®‰ ğŸ˜´",
        },
        witty_words: [],
        witty_comment: {
            prefix: 'Long time no see, ',
            back: 'Welcome back again,',
        },
    },
    covercolor: {
        enable: false
    },
    comment: {"avatar":"https://gravatar.com/avatar","commentBarrage":true},
    lightbox: 'null',
    right_menu: false,
    translate: {"translateDelay":0,"defaultEncoding":2},
    lure: false,
    expire: false,
};</script><!-- page-config head--><script id="config-diff">var PAGE_CONFIG = {
    is_post: true,
    is_page: false,
    is_home: false,
    page: '',
    toc: true,
    comment: true,
    ai_text: false,
    color: false,
}</script><meta name="generator" content="Hexo 7.3.0"></head><body id="body"><!-- universe--><canvas id="universe"></canvas><!-- background img--><!-- loading--><!-- console--><!-- sidebar--><div id="sidebar" style="zoom: 1;"><div id="menu-mask" style="display: none;"></div><div id="sidebar-menus"><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Archives</div><div class="length-num">18</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">7</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">27</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/recentcomments/"><div class="headline"> Comment</div><div class="length-num" id="waline_allcount"><i class="solitude fas fa-spinner fa-spin"></i></div><script pjax="pjax">(async () => {
  await fetch('/api/comment?type=recent&count=-1', {method: 'GET'}).then(async res => res.json())
    .then(async data => {
      document.querySelector('#waline_allcount').innerHTML = data.length
    })
})()</script></a></div></div></div><span class="sidebar-menu-item-title">Function</span><div class="sidebar-menu-item"><span class="darkmode_switchbutton menu-child" onclick="sco.switchDarkMode()"><i class="solitude fas fa-circle-half-stroke"></i><span>Display mode</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>æ–‡åº“</span></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="solitude  fas fa-folder-closed"></i><span>å…¨éƒ¨æ–‡ç« </span></a></li><li><a class="site-page child" href="/categories/"><i class="solitude  fas fa-clone"></i><span>å…¨éƒ¨åˆ†ç±»</span></a></li><li><a class="site-page child" href="/tags/"><i class="solitude  fas fa-tags"></i><span>å…¨éƒ¨æ ‡ç­¾</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>å…³äº</span></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="solitude  st-contacts-fill"></i><span>è‡ªæˆ‘ä»‹ç»</span></a></li></ul></div></div><span class="sidebar-menu-item-title">Tags</span><div class="card-tags"><div class="card-tag-cloud"><a href="/tags/web/">web<sup>1</sup></a><a href="/tags/http/">http<sup>1</sup></a><a href="/tags/dns/">dns<sup>2</sup></a><a href="/tags/debian/">debian<sup>1</sup></a><a href="/tags/53port/">53port<sup>1</sup></a><a href="/tags/socks5/">socks5<sup>1</sup></a><a href="/tags/Dante/">Dante<sup>1</sup></a><a href="/tags/vscode-python-conda/">vscode python conda<sup>1</sup></a><a href="/tags/bind9/">bind9<sup>1</sup></a><a href="/tags/conda/">conda<sup>1</sup></a><a href="/tags/algebra/">algebra<sup>1</sup></a><a href="/tags/mathematics/">mathematics<sup>1</sup></a><a href="/tags/hexo/">hexo<sup>2</sup></a><a href="/tags/Fonts/">Fonts<sup>1</sup></a><a href="/tags/OpenWRT/">OpenWRT<sup>1</sup></a><a href="/tags/server/">server<sup>1</sup></a><a href="/tags/vps/">vps<sup>1</sup></a><a href="/tags/theme/">theme<sup>1</sup></a><a href="/tags/ssl/">ssl<sup>1</sup></a><a href="/tags/AI-pytorch/">AI pytorch<sup>1</sup></a><a href="/tags/jottings/">jottings<sup>2</sup></a><a href="/tags/python/">python<sup>2</sup></a><a href="/tags/mysql/">mysql<sup>2</sup></a><a href="/tags/api/">api<sup>2</sup></a><a href="/tags/udesk/">udesk<sup>2</sup></a><a href="/tags/ydns/">ydns<sup>1</sup></a><a href="/tags/ddns/">ddns<sup>1</sup></a></div></div></div></div><!-- keyboard--><!-- righhtside--><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav class="show" id="nav"><div id="nav-group"><div id="blog_name"><a id="site-name" href="/" title="Back to home"><span class="title">ä¸»é¡µ</span><i class="solitude fas fa-home"></i></a></div><div id="page-name-mask"><div id="page-name"><a id="page-name-text" onclick="sco.toTop()">tansformer ä»å…¥é—¨åˆ°ä¸Šæ‰‹åªéœ€è¦çœ‹è¿™ä¸€ç¯‡</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>æ–‡åº“</span></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="solitude  fas fa-folder-closed"></i><span>å…¨éƒ¨æ–‡ç« </span></a></li><li><a class="site-page child" href="/categories/"><i class="solitude  fas fa-clone"></i><span>å…¨éƒ¨åˆ†ç±»</span></a></li><li><a class="site-page child" href="/tags/"><i class="solitude  fas fa-tags"></i><span>å…¨éƒ¨æ ‡ç­¾</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>å…³äº</span></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="solitude  st-contacts-fill"></i><span>è‡ªæˆ‘ä»‹ç»</span></a></li></ul></div></div></div><div id="nav-left"></div><div id="nav-right"><div class="nav-button" id="nav-totop" onclick="sco.toTop()"><a class="totopbtn"><i class="solitude fas fa-arrow-up"></i><span id="percent">0</span></a></div><div id="toggle-menu"><a class="site-page"><i class="solitude fas fa-bars"></i></a></div></div></div></nav><div class="coverdiv" id="coverdiv"><img class="nolazyload" id="post-cover" src="/2025/03/03/transformer1/image.png" alt="tansformer ä»å…¥é—¨åˆ°ä¸Šæ‰‹åªéœ€è¦çœ‹è¿™ä¸€ç¯‡"></div><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original" title="This article is a Original article, pay attention to the copyright.">Original</a><span class="post-meta-categories"><a class="post-meta-categories" href="/categories/AI/">AI</a></span><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI-pytorch/"><span class="tags-name tags-punctuation"><i class="solitude fas fa-hashtag"></i>AI pytorch</span></a></div></div></div></div><h1 class="post-title">tansformer ä»å…¥é—¨åˆ°ä¸Šæ‰‹åªéœ€è¦çœ‹è¿™ä¸€ç¯‡</h1><div id="post-meta"><div class="meta-secondline"><span class="post-meta-date" title="Posted on 2025-03-03 00:38:09"><i class="post-meta-icon solitude fas fa-calendar-days"></i><time datetime="2025-03-02T16:38:09.000Z">2025-03-02T16:38:09.000Z</time></span><span class="post-meta-date" title="Last updated on 2025-03-07 00:02:14"><i class="post-meta-icon solitude fas fa-arrows-rotate"></i><time datetime="2025-03-06T16:02:14.113Z">2025-03-06T16:02:14.113Z</time></span><span class="post-meta-position" title="The author's IP belongs toChina"><i class="post-meta-icon solitude fas fa-location-dot"></i><span>China</span></span><a class="post-meta-pv" href="/2025/03/03/transformer1/" title="PV"><i class="post-meta-icon solitude fas fa-fire-flame-curved"></i><span class="waline-pageview-count"><i class="solitude fas fa-spinner fa-spin"></i></span></a></div></div></div><div id="post-info-bottom"></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content article-container"><h1>tansformer ä»å…¥é—¨åˆ°ä¸Šæ‰‹åªéœ€è¦çœ‹è¿™ä¸€ç¯‡</h1>
<h2 id="tansformer-æ¶æ„è§£æ">tansformer æ¶æ„è§£æ</h2>
<h3 id="æ¶æ„æ€»è§ˆ">æ¶æ„æ€»è§ˆ</h3>
<p><img src= "/img/loading.gif" data-lazy-src="/2025/03/03/transformer1/image.png" alt="Alt text"></p>
<h3 id="è¯åµŒå…¥å±‚-Embeddings">è¯åµŒå…¥å±‚ Embeddings</h3>
<p>è¾“å…¥ token çš„ç´¢å¼•å¤„åœ¨ç»´åº¦ä¸ºè¯è¡¨å¤§å° d_vocab çš„ç©ºé—´é‡Œï¼Œæ¯ä¸ª token è¦ç»è¿‡è¯åµŒå…¥å±‚å˜æˆç»´åº¦ä¸º d_model çš„ç‰¹å¾å‘é‡ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_vocab</span>):</span><br><span class="line">        <span class="built_in">super</span>(Embeddings, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.lut(x) * math.sqrt(<span class="variable language_">self</span>.d_model)</span><br></pre></td></tr></table></figure>
<p>åœ¨åŸè®ºæ–‡ï¼Œç¼–ç å™¨è¾“å…¥ã€è§£ç å™¨è¾“å…¥ä»¥åŠè§£ç å™¨è¾“å‡ºæ˜¯ä½¿ç”¨ç›¸åŒå‚æ•°çš„ Embeddingsï¼Œä½†æ˜¯åœ¨æœ‰äº›åœ°æ–¹ï¼Œå‘ç°è§£ç å™¨è¾“å‡ºæ˜¯ä½¿ç”¨å•ç‹¬çš„ç”Ÿæˆå™¨ Generator ç»“æ„ï¼ŒåŸå› æ˜¯å› ä¸ºè§£ç å™¨è¾“å‡ºåœ¨åŠŸèƒ½ä¸Šæ˜¯è·å–ä¸€ä¸ªä»ç‰¹å¾å‘é‡åˆ°è¯è¡¨ç©ºé—´çš„æ¦‚ç‡æ˜ å°„ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Define standard linear + softmax generation step.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_vocab</span>):</span><br><span class="line">        <span class="built_in">super</span>(Generator, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.log_softmax(<span class="variable language_">self</span>.proj(x), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="ä½ç½®ç¼–ç -Positional-Encoding">ä½ç½®ç¼–ç  Positional Encoding</h3>
<p>ä¸ºäº†è®©æ¨¡å‹åˆ©ç”¨åºåˆ—çš„æ¬¡åºï¼Œå¿…é¡»æ³¨å…¥ä¸€äº›å…³äºåºåˆ—ä¸­çš„ç›¸å¯¹ä½ç½®æˆ–ç»å¯¹ä½ç½®çš„ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œåœ¨ encoder stack å’Œ decoder stack åº•éƒ¨çš„ input embedding ä¸­æ·»åŠ äº† positional encoding ã€‚positional encoding ä¸ embedding å…·æœ‰ç›¸åŒçš„ç»´åº¦ ï¼Œå› æ­¤å¯ä»¥å°†äºŒè€…ç›¸åŠ ã€‚</p>
<p>P_j=(PE_{(j,1)}, PE_{(j,2)}...PE_{(j,d_{model})}) \\
PE_{(j,2i)}=\sin(\frac{j}{10000^{2i/d_{model}}}), PE_{(j,2i+1)}=\cos(\frac{j}{10000^{2i/d_{model}}})
</p>
<p>å…¶ä¸­,j è¡¨ç¤º positionï¼Œi è¡¨ç¤ºç»´åº¦ã€‚å³ positional encoding çš„æ¯ä¸ªç»´åº¦å¯¹åº”äºä¸€ä¸ªæ­£å¼¦æ›²çº¿ï¼Œæ­£å¼¦æ›²çº¿çš„æ³¢é•¿ä» 2\pi ï¼ˆå½“ç»´åº¦ i=0 æ—¶ï¼‰åˆ° 10000 \times 2\pi ï¼ˆå½“ç»´åº¦ 2i=d_{model} æ—¶ï¼‰ã€‚</p>
<p>å¯¹äºä»»æ„å›ºå®šçš„åç§»é‡ kï¼ŒP_{j+k} å¯ä»¥è¡¨ç¤ºä¸º P_j çš„çº¿æ€§å‡½æ•°ï¼Œä¸”æ­£å¼¦ç‰ˆæœ¬çš„ positional embedding å¯ä»¥åº”ç”¨åˆ°è®­ç»ƒæœŸé—´ unseen çš„ä½ç½®ï¼Œä»è€Œå¯ä»¥è®©æ¨¡å‹æ¨æ–­å‡ºæ¯”è®­ç»ƒæœŸé—´é‡åˆ°çš„åºåˆ—é•¿åº¦æ›´é•¿çš„åºåˆ—ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;å®ç°Positional EncodingåŠŸèƒ½&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        ä½ç½®ç¼–ç å™¨çš„åˆå§‹åŒ–å‡½æ•°</span></span><br><span class="line"><span class="string">        :param d_model: è¯å‘é‡çš„ç»´åº¦ï¼Œä¸è¾“å…¥åºåˆ—çš„ç‰¹å¾ç»´åº¦ç›¸åŒï¼Œ512</span></span><br><span class="line"><span class="string">        :param dropout: ç½®é›¶æ¯”ç‡</span></span><br><span class="line"><span class="string">        :param max_len: å¥å­æœ€å¤§é•¿åº¦,5000</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># å¦‚æœd_modelä¸æ˜¯å¶æ•°é‚£ä¹ˆåé¢pe[:, 1::2]ä¼šå°‘ä¸€ä¸ªç»´åº¦ï¼Œèµ‹å€¼æŠ¥é”™ã€‚</span></span><br><span class="line">        <span class="keyword">assert</span> d_model % <span class="number">2</span> == <span class="number">0</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># åˆå§‹åŒ–ä¸€ä¸ªnn.Dropoutå±‚ï¼Œè®¾ç½®ç»™å®šçš„dropoutæ¯”ä¾‹</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># åˆå§‹åŒ–ä¸€ä¸ªä½ç½®ç¼–ç çŸ©é˜µ</span></span><br><span class="line">        <span class="comment"># (5000,512)çŸ©é˜µï¼Œä¿æŒæ¯ä¸ªä½ç½®çš„ä½ç½®ç¼–ç ï¼Œä¸€å…±5000ä¸ªä½ç½®ï¼Œæ¯ä¸ªä½ç½®ç”¨ä¸€ä¸ª512ç»´åº¦å‘é‡æ¥è¡¨ç¤ºå…¶ä½ç½®ç¼–ç </span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        <span class="comment"># å¶æ•°å’Œå¥‡æ•°åœ¨å…¬å¼ä¸Šæœ‰ä¸€ä¸ªå…±åŒéƒ¨åˆ†ï¼Œä½¿ç”¨logå‡½æ•°æŠŠæ¬¡æ–¹æ‹¿ä¸‹æ¥ï¼Œæ–¹ä¾¿è®¡ç®—</span></span><br><span class="line">        <span class="comment"># positionè¡¨ç¤ºçš„æ˜¯å­—è¯åœ¨å¥å­ä¸­çš„ç´¢å¼•ï¼Œå¦‚max_lenæ˜¯128ï¼Œé‚£ä¹ˆç´¢å¼•å°±æ˜¯ä»0ï¼Œ1ï¼Œ2ï¼Œ...,127</span></span><br><span class="line">        <span class="comment"># è®ºæ–‡ä¸­d_modelæ˜¯512ï¼Œ2iç¬¦å·ä¸­iä»0å–åˆ°255ï¼Œé‚£ä¹ˆ2iå¯¹åº”å–å€¼å°±æ˜¯0,2,4...510</span></span><br><span class="line">        <span class="comment"># (5000) -&gt; (5000,1)</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># è®¡ç®—ç”¨äºæ§åˆ¶æ­£ä½™å¼¦çš„ç³»æ•°ï¼Œç¡®ä¿ä¸åŒé¢‘ç‡æˆåˆ†åœ¨d_modelç»´ç©ºé—´å†…å‡åŒ€åˆ†å¸ƒ</span></span><br><span class="line">        div_term = torch.exp(</span><br><span class="line">            torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># æ ¹æ®ä½ç½®å’Œdiv_termè®¡ç®—æ­£å¼¦å’Œä½™å¼¦å€¼ï¼Œåˆ†åˆ«èµ‹å€¼ç»™peçš„å¶æ•°åˆ—å’Œå¥‡æ•°åˆ—</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(</span><br><span class="line">            position * div_term</span><br><span class="line">        )  <span class="comment"># ä»0å¼€å§‹åˆ°æœ€åé¢ï¼Œè¡¥é•¿ä¸º2ï¼Œå…¶å®ä»£è¡¨çš„å°±æ˜¯å¶æ•°ä½ç½®</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(</span><br><span class="line">            position * div_term</span><br><span class="line">        )  <span class="comment"># ä»1å¼€å§‹åˆ°æœ€åé¢ï¼Œè¡¥é•¿ä¸º2ï¼Œå…¶å®ä»£è¡¨çš„å°±æ˜¯å¥‡æ•°ä½ç½®</span></span><br><span class="line">        <span class="comment"># ä¸Šé¢ä»£ç è·å–ä¹‹åå¾—åˆ°çš„pe:[max_len * d_model]</span></span><br><span class="line">        <span class="comment"># ä¸‹é¢è¿™ä¸ªä»£ç ä¹‹åå¾—åˆ°çš„peå½¢çŠ¶æ˜¯ï¼š[1 * max_len * d_model]</span></span><br><span class="line">        <span class="comment"># å¤šå¢åŠ 1ç»´ï¼Œæ˜¯ä¸ºäº†é€‚åº”batch_size</span></span><br><span class="line">        <span class="comment"># (5000, 512) -&gt; (1, 5000, 512)</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># å°†è®¡ç®—å¥½çš„ä½ç½®ç¼–ç çŸ©é˜µæ³¨å†Œä¸ºæ¨¡å—ç¼“å†²åŒºï¼ˆbufferï¼‰ï¼Œè¿™æ„å‘³ç€å®ƒå°†æˆä¸ºæ¨¡å—çš„ä¸€éƒ¨åˆ†å¹¶éšæ¨¡å‹ä¿å­˜ä¸åŠ è½½ï¼Œä½†ä¸ä¼šè¢«è§†ä¸ºæ¨¡å‹å‚æ•°å‚ä¸åå‘ä¼ æ’­</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&quot;pe&quot;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):transformer1</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x: [seq_len, batch_size, d_model]  ç»è¿‡è¯å‘é‡çš„è¾“å…¥</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        x = (</span><br><span class="line">            x + <span class="variable language_">self</span>.pe[:, : x.size(<span class="number">1</span>)].clone().detach()</span><br><span class="line">        )  <span class="comment"># ç»è¿‡è¯å‘é‡çš„è¾“å…¥ä¸ä½ç½®ç¼–ç ç›¸åŠ </span></span><br><span class="line">        <span class="comment"># clone().detach() ç»„åˆèµ·æ¥çš„ä¸»è¦ä½œç”¨æ˜¯åˆ›å»ºä¸€ä¸ªä¸åŸå¼ é‡æ•°æ®ç›¸åŒï¼Œä½†ä¸å‚ä¸æ¢¯åº¦è®¡ç®—ä¸”å†…å­˜ç‹¬ç«‹çš„æ–°å¼ é‡</span></span><br><span class="line">        <span class="comment"># Dropoutå±‚ä¼šæŒ‰ç…§è®¾å®šçš„æ¯”ä¾‹éšæœºâ€œä¸¢å¼ƒâ€ï¼ˆç½®é›¶ï¼‰ä¸€éƒ¨åˆ†ä½ç½®ç¼–ç ä¸è¯å‘é‡ç›¸åŠ åçš„å…ƒç´ ï¼Œ</span></span><br><span class="line">        <span class="comment"># ä»¥æ­¤å¼•å…¥æ­£åˆ™åŒ–æ•ˆæœï¼Œé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆ</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.dropout(x)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="å¤šå¤´æ³¨æ„åŠ›">å¤šå¤´æ³¨æ„åŠ›</h3>
<p><img src= "/img/loading.gif" data-lazy-src="/2025/03/03/transformer1/image-1.png" alt="Alt text"></p>
<h4 id="æ³¨æ„åŠ›æœºåˆ¶-Scaled-Dot-Product-Attention">æ³¨æ„åŠ›æœºåˆ¶ Scaled Dot-Product Attention</h4>
<p>åŸè®ºæ–‡å« Scaled Dot-Product Attention ï¼ˆæ”¾ç¼©ç‚¹ç§¯æ³¨æ„åŠ›ï¼‰,æ¶æ„å¦‚ä¸Šå›¾å·¦ä¾§æ‰€ç¤ºã€‚</p>
<p>æ³¨æ„åŠ›æœºåˆ¶åŒ…æ‹¬ Query ( Q )ï¼ŒKey ( K )å’Œ Value ( V )ä¸‰ä¸ªç»„æˆéƒ¨åˆ†ï¼Œè¿™ä¸‰ä¸ªéƒ¨åˆ†ç”±W_Q,W_K,W_Vä¸‰ä¸ªçº¿æ€§å±‚ç”Ÿæˆï¼Œè¿™éƒ¨åˆ†åŸè®ºæ–‡æ¶æ„å›¾æ²¡æœ‰ç»™å‡ºï¼Œæ˜¯æˆ‘è‡ªå·±è¡¥å……çš„ï¼Œå…¬å¼å¦‚ä¸‹ï¼š</p>
<p>Q(len\_q, d\_k) = X_Q(len\_q, d_model) \cdot W_Q(d_model, d\_k) \\
K(len\_v, d\_k) = X_V(len\_v, d_model) \cdot W_K(d_model, d\_k) \\
V(len\_v, d\_v) = X_V(len\_v, d_model) \cdot W_V(d_model, d\_v)
</p>
<p>å…¶ä¸­ Q å’Œ K æœ‰ç›¸åŒçš„ç‰¹å¾ç»´åº¦ d_kï¼Œè€Œ K å’Œ V æœ‰ç›¸åŒçš„é•¿åº¦ç»´åº¦ len_vï¼Œå®ƒä»¬ç”±X_Q,X_Vä¸¤ä¸ªè¾“å…¥ç”Ÿæˆï¼Œå½“X_Q=X_Væ—¶è®¡ç®—çš„æ˜¯è‡ªæ³¨æ„åŠ›ï¼Œå½“å®ƒä»¬ä¸åŒæ—¶è®¡ç®—çš„å°±æ˜¯äº¤å‰æ³¨æ„åŠ›ã€‚</p>
<p>åœ¨å¾ˆå¤šä»£ç ä¸­W_Q,W_K,W_Vä¸‰ä¸ªçº¿æ€§å±‚æ˜¯é»˜è®¤æœ‰åç½®é¡¹çš„ï¼Œè¿™é‡Œä¸ºäº†ä¾¿äºç†è§£æ²¡æœ‰å†™å‡ºæ¥ã€‚</p>
<p>æ³¨æ„åŠ›è®¡ç®—å…¬å¼ä¸ºï¼š</p>
<p>Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V \\
Q \in \R ^{len_q \times d_k},K \in \R ^{len_v \times d_k},v \in \R ^{len_v \times d_v}, QK^T \in \R ^{len_q \times len_v}, Attention(Q,K,V) \in \R ^{len_q \times d_v}
</p>
<p>å†…ç§¯æ³¨æ„åŠ›ä½¿ç”¨äº† 1/\sqrt{d_k} çš„ç¼©æ”¾å› å­,å› ä¸ºç»´åº¦è¶Šå¤§ï¼Œåˆ™å†…ç§¯ä¸­ç´¯åŠ å’Œçš„é¡¹è¶Šå¤šï¼Œå†…ç§¯ç»“æœè¶Šå¤§ã€‚å¾ˆå¤§çš„æ•°å€¼ä¼šå¯¼è‡´ softmax å‡½æ•°ä½äºé¥±å’ŒåŒºé—´ï¼Œæ¢¯åº¦å‡ ä¹ä¸ºé›¶ã€‚</p>
<p>å› ä¸ºè¾“å‡ºç‰¹å¾ç»´åº¦ä¸º d_vï¼Œæ‰€ä»¥ä¸ºäº†æ·»åŠ æ®‹å·®ï¼Œé€šå¸¸è¦æ±‚ d_v=d_model æˆ–è€…ä¹Ÿå¯ä»¥å¼•å…¥ä¸€ä¸ªd_v \to d_modelçš„çº¿æ€§å±‚ã€‚</p>
<h4 id="å¤šå¤´æ³¨æ„åŠ›-Multi-Head-Attention">å¤šå¤´æ³¨æ„åŠ› Multi-Head Attention</h4>
<p>å¤šå¤´æ³¨æ„åŠ›å°† query, key, value çº¿æ€§æŠ•å½± h æ¬¡ï¼Œå…¶ä¸­æ¯æ¬¡çº¿æ€§æŠ•å½±éƒ½æ˜¯ä¸åŒçš„å¹¶ä¸”å°† query, key, value åˆ†åˆ«æŠ•å½±åˆ° d_k,d_k,d_v ç»´ã€‚ç„¶åï¼Œåœ¨æ¯ä¸ª query, key, value çš„æŠ•å½±åçš„ç‰ˆæœ¬ä¸Šï¼Œå¹¶è¡Œæ‰§è¡Œæ³¨æ„åŠ›å‡½æ•°ï¼Œæ¯ä¸ªæ³¨æ„åŠ›å‡½æ•°äº§ç”Ÿ d_v ç»´çš„ output ã€‚è¿™äº› output è¢«æ‹¼æ¥èµ·æ¥å¹¶å†æ¬¡æŠ•å½±ï¼Œäº§ç”Ÿ final output ï¼Œå¦‚ä¸Šå›¾å³ä¾§æ‰€ç¤ºã€‚</p>
<p>åŸè®ºæ–‡è¯´ï¼Œå¤šå¤´æ³¨æ„åŠ›å…è®¸æ¨¡å‹åœ¨æ¯ä¸ªä½ç½®è”åˆåœ°å…³æ³¨æ¥è‡ªä¸åŒå­ç©ºé—´çš„ä¿¡æ¯ã€‚å¦‚æœåªæœ‰å•ä¸ªæ³¨æ„åŠ›å¤´ï¼Œé‚£ä¹ˆå¹³å‡æ“ä½œä¼šæŠ‘åˆ¶è¿™ä¸€ç‚¹ã€‚</p>
<p>MultiHead(Q,K,V)=Concat(head_1,head_2 ... head_h)W^O \\
head_i=Attention(QW^Q_i,KW^K_i,VW^V_i) \\
W^Q_i \in \R ^{d_{model} \times d_k},W^K_i \in \R ^{d_{model} \times d_k},W^V_i \in \R ^{d_{model} \times d_v},W^O \in \R ^{(hd_v) \times d_{model}}
</p>
<p>å…¬å¼ä¸­ç»™å‡ºçš„æ˜¯æ›´ä¸€èˆ¬çš„å½¢å¼,å®é™…ä¸Šè¿™é‡Œçš„è¾“å…¥ Q å¯¹åº”ä¸Šæ–‡æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„X_Qï¼Œè¾“å…¥ K å’Œ V å¯¹åº”ä¸Šæ–‡æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„X_Vï¼Œå¦å¤–åŸè®ºæ–‡ä¸­ d*model=512ï¼Œ d_k=d_v=d*{model}/h=64ã€‚</p>
<p>åœ¨ Transformer ä¸­ä»¥ä¸‰ç§ä¸åŒçš„æ–¹å¼ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›ï¼š</p>
<ol>
<li>åœ¨ encoder-decoder attention å±‚ä¸­ï¼Œquery æ¥è‡ªäºå‰ä¸€ä¸ª decoder layerï¼Œkey å’Œ value æ¥è‡ªäº encoder çš„è¾“å‡ºã€‚è¿™å…è®¸ decoder ä¸­çš„æ¯ä¸ªä½ç½®å…³æ³¨ input åºåˆ—ä¸­çš„æ‰€æœ‰ä½ç½®ã€‚è¿™æ¨¡ä»¿äº† sequence-to-sequence æ¨¡å‹ä¸­å…¸å‹çš„ encoder-decoder attention æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>encoder åŒ…å«è‡ªæ³¨æ„åŠ›å±‚ã€‚åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¸­ï¼Œæ‰€æœ‰çš„ query, key, value éƒ½æ¥è‡ªäºåŒä¸€ä¸ªåœ°æ–¹ï¼ˆåœ¨è¿™ä¸ª case ä¸­ï¼Œå°±æ˜¯ encoder ä¸­å‰ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚encoder ä¸­çš„æ¯ä¸ªä½ç½®éƒ½å¯ä»¥å…³æ³¨ encoder ä¸Šä¸€å±‚ä¸­çš„æ‰€æœ‰ä½ç½®ã€‚</li>
<li>ç±»ä¼¼åœ°ï¼Œdecoder ä¸­çš„è‡ªæ³¨æ„åŠ›å±‚å…è®¸ decoder ä¸­çš„æ¯ä¸ªä½ç½®å…³æ³¨ decoder ä¸­æˆªè‡³åˆ°å½“å‰ä¸ºæ­¢ï¼ˆåŒ…å«å½“å‰ä½ç½®ï¼‰çš„æ‰€æœ‰ä½ç½®ã€‚æˆ‘ä»¬éœ€è¦é˜²æ­¢ decoder ä¸­çš„ä¿¡æ¯å‘å·¦æµåŠ¨ï¼Œä»è€Œä¿æŒè‡ªå›å½’ç‰¹æ€§ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ scaled dot-product attention å†…éƒ¨å±è”½æ‰ softmax input çš„æŸäº› value æ¥å®ç°è¿™ä¸€ç‚¹ï¼ˆå°†è¿™äº› value è®¾ç½®ä¸º -1e9,ä¸€ä¸ªå¾ˆå°çš„æ•°ï¼‰ï¼Œè¿™äº› value å¯¹åº”äºæ— æ•ˆè¿æ¥ illegal connectionã€‚</li>
</ol>
<h4 id="ä»£ç å®ç°-pytorch">ä»£ç å®ç° pytorch</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="string">&quot;Produce N identical layers.&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    query: (len_q, d_k)</span></span><br><span class="line"><span class="string">    key: (len_v, d_k)</span></span><br><span class="line"><span class="string">    value: (len_v, d_v)</span></span><br><span class="line"><span class="string">    mask: (len_q, len_v) fill true</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>))/math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask, -<span class="number">1e9</span>)</span><br><span class="line">    p_attn = torch.softmax(scores, dim = -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Take in model size and number of heads.</span></span><br><span class="line"><span class="string">        We assume d_v always equals d_k.</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.d_k = d_model // h</span><br><span class="line">        <span class="variable language_">self</span>.h = h</span><br><span class="line">        <span class="variable language_">self</span>.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        <span class="variable language_">self</span>.attn = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        query: (len_q, d_k)</span></span><br><span class="line"><span class="string">        key: (len_v, d_k)</span></span><br><span class="line"><span class="string">        value: (len_v, d_k)</span></span><br><span class="line"><span class="string">        mask: (len_q, len_v) fill true</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span></span><br><span class="line">        query, key, value = [</span><br><span class="line">            lin(x).view(nbatches, -<span class="number">1</span>, <span class="variable language_">self</span>.h, <span class="variable language_">self</span>.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">for</span> lin, x <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.linears, (query, key, value))</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch.</span></span><br><span class="line">        x, <span class="variable language_">self</span>.attn = attention(</span><br><span class="line">            query, key, value, mask=mask, dropout=<span class="variable language_">self</span>.dropout</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear.</span></span><br><span class="line">        x = (</span><br><span class="line">            x.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">            .contiguous()</span><br><span class="line">            .view(nbatches, -<span class="number">1</span>, <span class="variable language_">self</span>.h * <span class="variable language_">self</span>.d_k)</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">del</span> query</span><br><span class="line">        <span class="keyword">del</span> key</span><br><span class="line">        <span class="keyword">del</span> value</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linears[-<span class="number">1</span>](x)</span><br></pre></td></tr></table></figure>
<h3 id="ç¼–ç å™¨æ©ç -length-mask">ç¼–ç å™¨æ©ç  length mask</h3>
<p><img src= "/img/loading.gif" data-lazy-src="/2025/03/03/transformer1/image-3.png" alt="Alt text"><br>
å¯¹äºè¾“å…¥ xï¼Œå…¶åŸå§‹åºåˆ—é•¿åº¦æ˜¯é•¿çŸ­ä¸ä¸€çš„ï¼Œä½†æ˜¯ä¸ºäº†æ”¾å…¥å¼ é‡ä¸­è¿›è¡Œè®­ç»ƒï¼Œä¼šå¯¹çŸ­åºåˆ—å¡«è¡¥åˆ°æœ€å¤§é•¿åº¦ï¼Œä½¿è¾“å…¥åºåˆ—çš„é•¿åº¦ç»Ÿä¸€ã€‚è¿™æ ·åœ¨è®¡ç®—æ³¨æ„åŠ›æ—¶ï¼Œå¡«è¡¥çš„ç»´åº¦åº”è¯¥æ˜¯æ— æ•ˆçš„ä¸èƒ½å‚ä¸è®¡ç®—ã€‚</p>
<p>æ³¨æ„åŠ›è®¡ç®—ä¸­ socre çš„ç»´åº¦ä¸º(len_q, len_v),å› ä¸º torch.softmax(scores, dim = -1)è®¡ç®—æœ€åä¸€ä¸ªç»´åº¦åˆ—ï¼Œå¯¹ä¸åŒåˆ—ä¹‹é—´è¿›è¡Œ softmaxï¼Œæ‰€ä»¥å®é™…ä¸Šæ˜¯å¯¹æ¯è¡Œæ•°æ® softmaxã€‚è¿™æ ·è¦ä¿è¯æ¯è¡Œä¸èƒ½å…¨è¢« maskï¼Œé˜²æ­¢è®¡ç®—å‡ºç° NaNã€‚è¿™æ ·å¯¹ len_v åˆ—ç»´åº¦è¿›è¡Œ mask å°±è¡Œäº†ï¼Œlen_q å¯¹åº”éƒ¨åˆ†æœ€ç»ˆè®¡ç®— loss æ—¶ä¼šè¢«è¿‡æ»¤æ‰ï¼Œæ²¡æœ‰å½±å“ã€‚</p>
<p>è¿™é‡Œé¢„æœŸçš„è¾“å…¥ batch_mask (N, len) æ˜¯å·²ç»å¤„ç†å¥½çš„è¾“å…¥åºåˆ—çš„æ©ç ï¼ˆTrue ä»£ç å¡«å……éƒ¨åˆ†ï¼‰ï¼Œåªéœ€è¦åœ¨æ‰¹æ¬¡ç»´åº¦Nå’Œåºåˆ—é•¿åº¦ç»´åº¦lenä¹‹é—´å¡«å……ä¸€ä¸ªç»´åº¦ï¼Œé‚£ä¹ˆåœ¨ scores.masked_fill(mask, -1e9) æ—¶ï¼Œæœ€åç»´åº¦ len_v çš„æ©ç ä¼šè‡ªåŠ¨åœ¨å‰é¢ç»´åº¦ len_q ä¸Šå¹¿æ’­ï¼Œè€Œä¸ç”¨è€ƒè™‘ len_q çš„é•¿åº¦å˜åŒ–ï¼Œè¿™æ ·è¿™ä¸ªæ©ç æ—¢å¯ä»¥åœ¨ç¼–ç å™¨è®¡ç®—è‡ªæ³¨æ„åŠ›æ—¶ç”¨åˆ°ï¼Œåˆå¯ä»¥ç”¨åœ¨è®¡ç®—äº¤å‰æ³¨æ„åŠ›æ—¶ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">len_mask</span>(<span class="params">batch_mask</span>) -&gt; torch.Tensor:</span><br><span class="line">    <span class="keyword">return</span> batch_mask.clone().unsqueeze(-<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># è¿™é‡Œç»™å‡ºäº†ç¬¬äºŒä¸­æŠŠ len_v ç»´åº¦çš„æ©ç å¹¿æ’­åˆ°æ–°å¢çš„ len_q ç»´åº¦çš„æ–¹æ³•ã€‚</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">len_mask2</span>(<span class="params">batch_mask, len_q</span>) -&gt; torch.Tensor:</span><br><span class="line">    <span class="keyword">return</span> batch_mask.clone().unsqueeze(-<span class="number">2</span>).expand(*batch_mask.shape, len_q).transpose(-<span class="number">1</span>, -<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h3 id="è§£ç å™¨æ©ç -causal-mask">è§£ç å™¨æ©ç  causal mask</h3>
<p>æˆ‘ä»¬ç»™æ¨¡å‹çš„å®Œæ•´è¾“å…¥ä¸º (x_1, x_2...x_n) ï¼Œä½†æ˜¯é¢„æµ‹ x_i æ—¶åªç”¨åˆ°äº†å‰ i-1 ä¸ªè¾“å…¥çš„ä¿¡æ¯ï¼Œä¹Ÿå°±æ˜¯è¯´åé¢çš„ä¿¡æ¯æˆ‘ä»¬åº”å½“ mask æ‰ï¼Œåªèƒ½æ ¹æ®å†å²ä¿¡æ¯æ¥é¢„æµ‹å½“å‰ä½ç½®çš„è¾“å‡ºã€‚è¿™å¾ˆå¥½ç†è§£ï¼Œå‡å¦‚æ¨¡å‹çš„è¾“å…¥ä¸ºâ€œæ—©ä¸Šå¥½â€ï¼Œé‚£ä¹ˆåº”è¯¥é€šè¿‡â€œæ—©â€æ¥é¢„æµ‹â€œä¸Šâ€ï¼Œé€šè¿‡â€œæ—©ä¸Šâ€æ¥é¢„æµ‹â€œå¥½â€ï¼Œè€Œä¸æ˜¯é€šè¿‡â€œæ—©ä¸Šå¥½â€æ¥é¢„æµ‹â€œä¸Šâ€æˆ–è€…â€œå¥½â€ï¼Œå› ä¸ºè¿™ç›¸å½“äºç›´æ¥æŠŠ ground truth å‘Šè¯‰æ¨¡å‹äº†ã€‚</p>
<p><img src= "/img/loading.gif" data-lazy-src="/2025/03/03/transformer1/image-2.png" alt="Alt text"><br>
åœ¨è§£ç å™¨çš„è‡ªæ³¨æ„åŠ›å±‚ï¼Œsocre çš„ç»´åº¦ä¸º (len_q, len_v)ï¼Œæˆ‘ä»¬åªéœ€è¦å¦‚ä¸Šå›¾ç”Ÿæˆä¸€ä¸ªä¸Šä¸‰è§’çŸ©é˜µï¼Œå°†å¯¹è§’çº¿ä»¥ä¸Šçš„éƒ¨åˆ† mask å°±è¡Œäº†ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">subsequent_mask</span>(<span class="params">size</span>):</span><br><span class="line">    <span class="string">&quot;Mask out subsequent positions.&quot;</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=<span class="number">1</span>).<span class="built_in">type</span>(</span><br><span class="line">        torch.uint8</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> subsequent_mask == <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="æ ‡å‡†åŒ–">æ ‡å‡†åŒ–</h3>
<p>è¿™é‡Œå¯¹æœ«å°¾ç»´åº¦ä¹Ÿå°±æ˜¯ç‰¹å¾ç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–ï¼Œæ ‡å‡†åŒ–åè¿˜é€šè¿‡äº†ä¸€ä¸ªçº¿æ€§å±‚ã€‚</p>
<p>è¿™é‡Œåˆ†åˆ«å¯¹çº¿æ€§å±‚çš„æƒé‡åˆå§‹åŒ–ä¸º 1ï¼Œåç½®åˆå§‹åŒ–ä¸º 0ã€‚è€Œ nn.liner é»˜è®¤ä½¿ç”¨ Kaiming åˆå§‹åŒ–æƒé‡ï¼Œå‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–åç½®ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Construct a layernorm module (See citation for details).&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        <span class="variable language_">self</span>.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        <span class="variable language_">self</span>.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.a_2 * (x - mean) / (std + <span class="variable language_">self</span>.eps) + <span class="variable language_">self</span>.b_2</span><br></pre></td></tr></table></figure>
<h3 id="æ®‹å·®è¿æ¥-residual-connections">æ®‹å·®è¿æ¥ residual connections</h3>
<p>è¿™é‡Œå¯¹ä¼ å…¥çš„å­å±‚æ ‡å‡†åŒ–åæ·»åŠ æ®‹å·®ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SublayerConnection</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.norm = LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line">        <span class="string">&quot;Apply residual connection to any sublayer with the same size.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> x + <span class="variable language_">self</span>.dropout(sublayer(<span class="variable language_">self</span>.norm(x)))</span><br></pre></td></tr></table></figure>
<h3 id="é€ä½ç½®å‰é¦ˆç½‘ç»œ-Position-wise-Feed-Forward-Networks">é€ä½ç½®å‰é¦ˆç½‘ç»œ Position-wise Feed-Forward Networks</h3>
<p>FFN(x)=max(0, xW_1+b_1)W_2+b_2
</p>
<p>å…ˆå°† x ä» d_model=512 ç»´æŠ•å½±åˆ° dff=2048 ç»´ï¼Œå†æŠ•å½±å› d_model=512 ç»´ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Implements FFN equation.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        <span class="variable language_">self</span>.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.w_2(<span class="variable language_">self</span>.dropout(<span class="variable language_">self</span>.w_1(x).relu()))</span><br></pre></td></tr></table></figure>
<h3 id="ç¼–ç å™¨-Ecoder">ç¼–ç å™¨ Ecoder</h3>
<p>å¯¹äºå•ä¸ªç¼–ç å™¨ï¼Œå…ˆé€šè¿‡æ®‹å·®è¿æ¥åŒ…è£¹çš„ self_attn è‡ªæ³¨æ„åŠ›å±‚ï¼Œå†é€šè¿‡æ®‹å·®è¿æ¥åŒ…è£¹çš„ feed_forward é€ä½ç½®å‰é¦ˆç½‘ç»œå±‚ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Encoder is made up of self-attn and feed forward (defined below)&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, self_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.self_attn = self_attn</span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = feed_forward</span><br><span class="line">        <span class="variable language_">self</span>.sublayer = clones(SublayerConnection(d_model, dropout), <span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.size = d_model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (left) for connections.&quot;</span></span><br><span class="line">        x = <span class="variable language_">self</span>.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: <span class="variable language_">self</span>.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.sublayer[<span class="number">1</span>](x, <span class="variable language_">self</span>.feed_forward)</span><br></pre></td></tr></table></figure>
<p>å®Œæ•´ç¼–ç å™¨å°±æ˜¯æŠŠä¸Šé¢çš„ç¼–ç é‡å¤ N å±‚ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªæ ‡å‡†åŒ–å±‚ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Core encoder is a stack of N layers&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.layers = clones(layer, N)</span><br><span class="line">        <span class="variable language_">self</span>.norm = LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;Pass the input (and mask) through each layer in turn.&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.norm(x)</span><br></pre></td></tr></table></figure>
<h3 id="è§£ç å™¨-Decoder">è§£ç å™¨ Decoder</h3>
<p>å•ä¸ªè§£ç å™¨é‡Œæœ‰ä¸¤ä¸ªæ³¨æ„åŠ›å±‚ï¼Œç¬¬ä¸€ä¸ªæ³¨æ„åŠ›å±‚å…ˆå¯¹è¾“å‡ºç»“æœè®¡ç®—è‡ªæ³¨æ„åŠ›ï¼Œå†å°†ç»“æœè¾“å‡ºåˆ°ç¬¬äºŒä¸ªæ³¨æ„åŠ›å±‚ä¸ç¼–ç å™¨çš„è¾“å‡ºç»“æœä¸€èµ·è®¡ç®—äº¤å‰æ³¨æ„åŠ›ï¼Œéšåè¿›å…¥ä¸€ä¸ªé€ä½ç½®å‰é¦ˆç½‘ç»œå±‚ï¼Œè¿™ä¸‰å±‚éƒ½ä½¿ç”¨å‰é¢å®šä¹‰çš„æ®‹å·®è¿æ¥åŒ…è£¹ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Decoder is made of self-attn, src-attn, and feed forward (defined below)&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, src_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.size = size</span><br><span class="line">        <span class="variable language_">self</span>.self_attn = self_attn</span><br><span class="line">        <span class="variable language_">self</span>.src_attn = src_attn</span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = feed_forward</span><br><span class="line">        <span class="variable language_">self</span>.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (right) for connections.&quot;</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = <span class="variable language_">self</span>.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: <span class="variable language_">self</span>.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = <span class="variable language_">self</span>.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: <span class="variable language_">self</span>.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.sublayer[<span class="number">2</span>](x, <span class="variable language_">self</span>.feed_forward)</span><br></pre></td></tr></table></figure>
<p>å®Œæ•´è§£ç å™¨å°±æ˜¯æŠŠä¸Šé¢çš„è§£ç å±‚é‡å¤ N å±‚ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªæ ‡å‡†åŒ–å±‚ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Generic N layer decoder with masking.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.layers = clones(layer, N)</span><br><span class="line">        <span class="variable language_">self</span>.norm = LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.norm(x)</span><br></pre></td></tr></table></figure>
<h3 id="Transformer">Transformer</h3>
<p><img src= "/img/loading.gif" data-lazy-src="/2025/03/03/transformer1/image.png" alt="Alt text"><br>
æŠŠä»¥ä¸Šç»„ä»¶ç»„åˆåˆ°ä¸€èµ·æ„æˆ Transformer æ¶æ„ï¼Œå…¶ä¸­ç¼–ç å™¨å’Œè§£ç å™¨çš„æ©ç ç”Ÿæˆä»£ç æ²¡æœ‰åŠ è¿›å»ï¼Œå› ä¸ºæ©ç æ˜¯è¦éšæ•°æ®ä¸€èµ·åˆå§‹åŒ–çš„ï¼Œå¹¶ä¸å’Œç½‘ç»œç»“æ„ä¸€èµ·åˆå§‹åŒ–ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A standard Transformer architecture.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, src_vocab, tgt_vocab, N=<span class="number">6</span>, d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        c = copy.deepcopy</span><br><span class="line">        attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">        ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">        position = PositionalEncoding(d_model, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.encoder = Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)</span><br><span class="line">        <span class="variable language_">self</span>.decoder = Decoder(</span><br><span class="line">            DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.src_embed = nn.Sequential(Embeddings(d_model, src_vocab), c(position))</span><br><span class="line">        <span class="variable language_">self</span>.tgt_embed = nn.Sequential(Embeddings(d_model, tgt_vocab), c(position))</span><br><span class="line">        <span class="variable language_">self</span>.generator = Generator(d_model, tgt_vocab)</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> <span class="variable language_">self</span>.parameters():</span><br><span class="line">            <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">                nn.init.xavier_uniform_(p)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, tgt, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;Take in and process masked src and target sequences.&quot;</span></span><br><span class="line">        out = <span class="variable language_">self</span>.decode(<span class="variable language_">self</span>.encode(src, src_mask), src_mask, tgt, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.generator(out)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, src, src_mask</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.encoder(<span class="variable language_">self</span>.src_embed(src), src_mask)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, memory, src_mask, tgt, tgt_mask</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.decoder(<span class="variable language_">self</span>.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure>
<h3 id="æµ‹è¯•">æµ‹è¯•</h3>
<p>æµ‹è¯•ä¸€ä¸‹æ˜¯å¦èƒ½è·‘é€šï¼Œæµ‹è¯•æ²¡æœ‰é—®é¢˜ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">inference_test</span>():</span><br><span class="line">    test_model = Transformer(<span class="number">11</span>, <span class="number">11</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># test_model.eval() è¯„ä¼°æ¨¡å¼ï¼Œæ¯”å¦‚ Dropout å±‚åœ¨è¯„ä¼°å’Œè®­ç»ƒæ—¶è¡¨ç°ä¸åŒã€‚</span></span><br><span class="line">    test_model.<span class="built_in">eval</span>()</span><br><span class="line">    src = torch.LongTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]])</span><br><span class="line">    src_mask = torch.ones(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">    src_mask = src_mask == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    memory = test_model.encode(src, src_mask)</span><br><span class="line">    ys = torch.zeros(<span class="number">1</span>, <span class="number">1</span>).type_as(src)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">9</span>):</span><br><span class="line">        out = test_model.decode(memory, src_mask, ys, subsequent_mask(ys.size(<span class="number">1</span>)))</span><br><span class="line">        prob = test_model.generator(out[:, -<span class="number">1</span>])</span><br><span class="line">        _, next_word = torch.<span class="built_in">max</span>(prob, dim=<span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat(</span><br><span class="line">            [ys, torch.empty(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Example Untrained Model Prediction:&quot;</span>, ys)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_tests</span>():</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        inference_test()</span><br></pre></td></tr></table></figure>
<h2 id="è®­ç»ƒç¬¬ä¸€ä¸ªæ¨¡å‹">è®­ç»ƒç¬¬ä¸€ä¸ªæ¨¡å‹</h2>
<p>è¿™é‡Œæˆ‘åœ¨ç½‘ä¸Šæ‰¾åˆ°äº†é˜¿é‡Œäº‘çš„å¤©æ± æ•°æ®é›†æ‰“æ¦œæŒ‘æˆ˜èµ›ï¼Œå¹¶é€‰æ‹©äº†ä¸­æ–‡åŒ»ç–—ä¿¡æ¯å¤„ç†è¯„æµ‹åŸºå‡† CBLUE ä¸­çš„ä¸­æ–‡åŒ»å­¦å‘½åå®ä½“è¯†åˆ« V2ï¼ˆCMeEE-V2ï¼‰ä»»åŠ¡ã€‚</p>
<h3 id="ä¸‹è½½æ•°æ®">ä¸‹è½½æ•°æ®</h3>
<p>æ•°æ®å¥½åƒä¸‹è½½è¦ç”³è¯·ï¼ŒæŒºéº»çƒ¦ï¼Œæˆ‘ç›´æ¥åœ¨ huggingface ä¸Šæ‰¾åˆ°äº†ç›¸åŒæ•°æ®é›†ã€‚</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># å…ˆé…ç½® git lfs ,ä¸ç„¶å¤§æ–‡ä»¶ä¸ä¼šå…‹éš†ä¸‹æ¥</span><br><span class="line">conda install git-lfs</span><br><span class="line">git lfs install</span><br><span class="line"># å¯¹äºå¤§äº5Gçš„huggingfaceæ•°æ®ï¼Œè¿˜éœ€è¦</span><br><span class="line">pip install huggingface_hub</span><br><span class="line">huggingface-cli lfs-enable-largefiles .</span><br><span class="line"># å…‹éš†æ•°æ®</span><br><span class="line">git clone https://huggingface.co/datasets/Rosenberg/CMeEE-V2</span><br></pre></td></tr></table></figure>
<h3 id="è®­ç»ƒ">è®­ç»ƒ</h3>
<p>å°†ä¹‹å‰çš„æ¨¡å‹éƒ¨åˆ†çš„ä»£ç æ•´ç†åˆ° <a target="_blank" rel="noopener" href="http://transformer.py">transformer.py</a> æ–‡ä»¶ä¸­ï¼Œç„¶åå¯¼å…¥é‡Œé¢çš„ç±»å’Œå‡½æ•°ã€‚<br>
æµ‹è¯•é€šè¿‡ï¼Œå¯ä»¥æ­£å¸¸è®­ç»ƒã€‚</p>
<p>æˆ‘è®¾ç½® N=2, d_model=128, d_ff=512, batch_size=5 ï¼Œå·²ç»æŒºå°çš„äº†ï¼Œè®­ç»ƒä¸€ä¸ªä½†æ˜¯è¿˜æ˜¯éœ€è¦4ä¸ªGçš„æ˜¾å­˜ï¼Œä»¥åŠæ¯è®­ç»ƒä¸€ä¸ªepochéœ€è¦2.9å°æ—¶çš„æ—¶é—´ã€‚æ—¶é—´å’Œæœ¬åœ°ç”µè„‘èµ„æºå®åœ¨ä¸å¤ªå¤Ÿç”¨ã€‚</p>
<p>è®­ç»ƒç»“æœå’Œä»£ç å·²ä¸Šä¼ gitï¼š<br>
<a target="_blank" rel="noopener" href="https://github.com/hs3434/CMeEE-V2">https://github.com/hs3434/CMeEE-V2</a></p>
<p>ä»lossæ¥çœ‹ï¼Œæ˜¯éšç€è®­ç»ƒä¸‹é™çš„ã€‚<br>
<img src= "/img/loading.gif" data-lazy-src="/2025/03/03/transformer1/loss.svg" alt="Alt text"><br>
<img src= "/img/loading.gif" data-lazy-src="/2025/03/03/transformer1/Average_Loss.svg" alt="Alt text"></p>
</article><div class="post-copyright"><div class="post-copyright__author_group"><div class="post-copyright__author_img"><img class="post-copyright__author_img_front" src= "/img/loading.gif" data-lazy-src="/img/logo.png"></div><div class="post-copyright__author_name">Shane Hu</div><div class="post-copyright__author_desc">Welcome, friends!</div></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div id="quit-box" onclick="RemoveRewardMask()"></div></div></div><div class="social-share"><a class="social-share-ico icon-qq" target="_blank" rel="noopener" href="https://connect.qq.com/widget/shareqq/index.html?url=https%3A%2F%2Fhs3434.github.io%2Fhs3434.github.io%2F2025%2F03%2F03%2Ftransformer1%2F&amp;title=tansformer%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E4%B8%8A%E6%89%8B%E5%8F%AA%E9%9C%80%E8%A6%81%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87&amp;desc=undefined&amp;summary=undefined&amp;site=tansformer%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E4%B8%8A%E6%89%8B%E5%8F%AA%E9%9C%80%E8%A6%81%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87&amp;pics=2025%2F03%2F03%2Ftransformer1%2Fimage.png" title="Share to QQ"><i class="solitude fab fa-qq"></i></a><a class="social-share-ico icon-weibo" target="_blank" rel="noopener" href="http://service.weibo.com/share/share.php?url=https%3A%2F%2Fhs3434.github.io%2Fhs3434.github.io%2F2025%2F03%2F03%2Ftransformer1%2F&amp;title=tansformer%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E4%B8%8A%E6%89%8B%E5%8F%AA%E9%9C%80%E8%A6%81%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87&amp;pic=2025%2F03%2F03%2Ftransformer1%2Fimage.png" title="Share to Weibo"><i class="solitude fab fa-weibo"></i></a><a class="social-share-ico icon-twitter" target="_blank" rel="noopener" href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fhs3434.github.io%2Fhs3434.github.io%2F2025%2F03%2F03%2Ftransformer1%2F&amp;text=tansformer%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E4%B8%8A%E6%89%8B%E5%8F%AA%E9%9C%80%E8%A6%81%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87" title="Share to Twitter"><i class="solitude fab fa-twitter"></i></a><a class="social-share-ico icon-facebook" target="_blank" rel="noopener" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fhs3434.github.io%2Fhs3434.github.io%2F2025%2F03%2F03%2Ftransformer1%2F" title="Share to Facebook"><i class="solitude fab fa-facebook"></i></a><a class="social-share-ico icon-telegram" target="_blank" rel="noopener" href="https://t.me/share/url?url=https%3A%2F%2Fhs3434.github.io%2Fhs3434.github.io%2F2025%2F03%2F03%2Ftransformer1%2F&amp;text=tansformer%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E4%B8%8A%E6%89%8B%E5%8F%AA%E9%9C%80%E8%A6%81%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87" title="Share to Telegram"><i class="solitude fab fa-telegram"></i></a><a class="social-share-ico icon-whatsapp" target="_blank" rel="noopener" href="https://api.whatsapp.com/send?text=tansformer%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E4%B8%8A%E6%89%8B%E5%8F%AA%E9%9C%80%E8%A6%81%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87 https%3A%2F%2Fhs3434.github.io%2Fhs3434.github.io%2F2025%2F03%2F03%2Ftransformer1%2F" title="Share to WhatsApp"><i class="solitude fab fa-whatsapp"></i></a><a class="social-share-ico icon-linkedin" target="_blank" rel="noopener" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3A%2F%2Fhs3434.github.io%2Fhs3434.github.io%2F2025%2F03%2F03%2Ftransformer1%2F&amp;title=tansformer%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E4%B8%8A%E6%89%8B%E5%8F%AA%E9%9C%80%E8%A6%81%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87&amp;summary=undefined&amp;source=tansformer%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E4%B8%8A%E6%89%8B%E5%8F%AA%E9%9C%80%E8%A6%81%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87" title="Share to LinkedIn"><i class="solitude fab fa-linkedin"></i></a><div class="social-share-ico icon-link" onclick="utils.copy(&quot;https://hs3434.github.io/hs3434.github.io/2025/03/03/transformer1/&quot;)" title="Share to Link"><i class="solitude fas fa-link"></i></div><div class="social-share-ico icon-qrcode" title="Share to QR code"><i class="solitude fas fa-qrcode"></i><div class="share-main"><div class="share-main-all"><div id="qrcode"></div><div class="reward-dec">Share to QR code</div></div></div><script pjax>typeof QRCode === 'function' && new QRCode(document.getElementById("qrcode"), {
    text: 'https://hs3434.github.io/hs3434.github.io/2025/03/03/transformer1/',
    correctLevel : QRCode.CorrectLevel.L
});
window.addEventListener('DOMContentLoaded', () => {
    new QRCode(document.getElementById("qrcode"), {
        text: 'https://hs3434.github.io/hs3434.github.io/2025/03/03/transformer1/',
        correctLevel : QRCode.CorrectLevel.L
    });
});
</script></div></div><div class="post-copyright__notice"><span class="post-copyright-info">This piece of writing is an original article, utilizing the<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-hans">CC BY-NC-SA 4.0</a>Agreement. For complete reproduction, please acknowledge the source as Courtesy of<a href="/">Blog of Shane Hu</a></span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI-pytorch/"><span class="tags-punctuation"><i class="solitude fas fa-hashtag"></i>AI pytorch<span class="tagsPageCount">1</span></span></a></div></div></div><nav class="needEndHide pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/03/06/250306/"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">vscode æ‰“å¼€powershellå‡ºç°ç¼–ç æŠ¥é”™é—®é¢˜è§£å†³</div></div></a></div><div class="next-post pull-right"><a href="/2024/06/20/2406202/"><div class="pagination-info"><div class="label">Next</div><div class="next_info">ä½¿ç”¨ dnscrypt-proxy è§£å†³ dns è¢«åŠ«æŒæ±¡æŸ“é—®é¢˜</div></div></a></div></nav><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="solitude fas fa-comment"></i><span>  Comment</span><span class="count"> (<span class="waline-comment-count"><i class="solitude fas fa-spinner fa-spin"></i></span>)</span></div></div><div class="comment-wrap"><div id="waline-wrap"></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><div class="top-group"><div class="sayhi" id="sayhi" onclick="sco.changeWittyWord()"></div></div></div><div class="avatar"><img alt="Avatar" src= "/img/loading.gif" data-lazy-src="/img/head_porttrait.jpeg"></div><div class="description"></div><div class="bottom-group"><span class="left"><div class="name">Shane Hu</div><div class="desc">é•¿é£ç ´æµªä¼šæœ‰æ—¶ï¼Œç›´æŒ‚äº‘å¸†æµæ²§æµ·ã€‚</div></span><div class="social-icons is-center"><a class="social-icon" target="_blank" rel="noopener" href="https://github.com/hs3434" title="Github"><i class="solitude  st-github-line "></i></a></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="solitude fas fa-bars"></i><span>Table of contents</span></div><div class="toc-content" id="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">tansformer ä»å…¥é—¨åˆ°ä¸Šæ‰‹åªéœ€è¦çœ‹è¿™ä¸€ç¯‡</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#tansformer-%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90"><span class="toc-text">tansformer æ¶æ„è§£æ</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84%E6%80%BB%E8%A7%88"><span class="toc-text">æ¶æ„æ€»è§ˆ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E5%B5%8C%E5%85%A5%E5%B1%82-Embeddings"><span class="toc-text">è¯åµŒå…¥å±‚ Embeddings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-Positional-Encoding"><span class="toc-text">ä½ç½®ç¼–ç  Positional Encoding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-text">å¤šå¤´æ³¨æ„åŠ›</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-Scaled-Dot-Product-Attention"><span class="toc-text">æ³¨æ„åŠ›æœºåˆ¶ Scaled Dot-Product Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B-Multi-Head-Attention"><span class="toc-text">å¤šå¤´æ³¨æ„åŠ› Multi-Head Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-pytorch"><span class="toc-text">ä»£ç å®ç° pytorch</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8%E6%8E%A9%E7%A0%81-length-mask"><span class="toc-text">ç¼–ç å™¨æ©ç  length mask</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8%E6%8E%A9%E7%A0%81-causal-mask"><span class="toc-text">è§£ç å™¨æ©ç  causal mask</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-text">æ ‡å‡†åŒ–</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5-residual-connections"><span class="toc-text">æ®‹å·®è¿æ¥ residual connections</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%90%E4%BD%8D%E7%BD%AE%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C-Position-wise-Feed-Forward-Networks"><span class="toc-text">é€ä½ç½®å‰é¦ˆç½‘ç»œ Position-wise Feed-Forward Networks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-Ecoder"><span class="toc-text">ç¼–ç å™¨ Ecoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8-Decoder"><span class="toc-text">è§£ç å™¨ Decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer"><span class="toc-text">Transformer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95"><span class="toc-text">æµ‹è¯•</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9E%8B"><span class="toc-text">è®­ç»ƒç¬¬ä¸€ä¸ªæ¨¡å‹</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-text">ä¸‹è½½æ•°æ®</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-text">è®­ç»ƒ</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="solitude fas fa-map"></i><span>New posts</span></div><div class="aside-list"><a class="aside-list-item" href="/2025/03/06/250306/" title="vscode æ‰“å¼€powershellå‡ºç°ç¼–ç æŠ¥é”™é—®é¢˜è§£å†³"><div class="thumbnail"><img alt="vscode æ‰“å¼€powershellå‡ºç°ç¼–ç æŠ¥é”™é—®é¢˜è§£å†³" src= "/img/loading.gif" data-lazy-src="/img/default.png"></div><div class="content"><span class="title" href="/2025/03/06/250306/" title="vscode æ‰“å¼€powershellå‡ºç°ç¼–ç æŠ¥é”™é—®é¢˜è§£å†³">vscode æ‰“å¼€powershellå‡ºç°ç¼–ç æŠ¥é”™é—®é¢˜è§£å†³</span><span class="categories" href="/2025/03/06/250306/">vscode</span></div></a><a class="aside-list-item" href="/2025/03/03/transformer1/" title="tansformer ä»å…¥é—¨åˆ°ä¸Šæ‰‹åªéœ€è¦çœ‹è¿™ä¸€ç¯‡"><div class="thumbnail"><img alt="tansformer ä»å…¥é—¨åˆ°ä¸Šæ‰‹åªéœ€è¦çœ‹è¿™ä¸€ç¯‡" src= "/img/loading.gif" data-lazy-src="/2025/03/03/transformer1/image.png"></div><div class="content"><span class="title" href="/2025/03/03/transformer1/" title="tansformer ä»å…¥é—¨åˆ°ä¸Šæ‰‹åªéœ€è¦çœ‹è¿™ä¸€ç¯‡">tansformer ä»å…¥é—¨åˆ°ä¸Šæ‰‹åªéœ€è¦çœ‹è¿™ä¸€ç¯‡</span><span class="categories" href="/2025/03/03/transformer1/">AI</span></div></a><a class="aside-list-item" href="/2024/06/20/2406202/" title="ä½¿ç”¨ dnscrypt-proxy è§£å†³ dns è¢«åŠ«æŒæ±¡æŸ“é—®é¢˜"><div class="thumbnail"><img alt="ä½¿ç”¨ dnscrypt-proxy è§£å†³ dns è¢«åŠ«æŒæ±¡æŸ“é—®é¢˜" src= "/img/loading.gif" data-lazy-src="/img/default.png"></div><div class="content"><span class="title" href="/2024/06/20/2406202/" title="ä½¿ç”¨ dnscrypt-proxy è§£å†³ dns è¢«åŠ«æŒæ±¡æŸ“é—®é¢˜">ä½¿ç”¨ dnscrypt-proxy è§£å†³ dns è¢«åŠ«æŒæ±¡æŸ“é—®é¢˜</span><span class="categories" href="/2024/06/20/2406202/">jottings</span></div></a><a class="aside-list-item" href="/2024/06/20/240620/" title="debian ä½¿ç”¨ dnscrypt-proxy å 53 ç«¯å£è¢«  1/init systemd å ç”¨é—®é¢˜"><div class="thumbnail"><img alt="debian ä½¿ç”¨ dnscrypt-proxy å 53 ç«¯å£è¢«  1/init systemd å ç”¨é—®é¢˜" src= "/img/loading.gif" data-lazy-src="/img/default.png"></div><div class="content"><span class="title" href="/2024/06/20/240620/" title="debian ä½¿ç”¨ dnscrypt-proxy å 53 ç«¯å£è¢«  1/init systemd å ç”¨é—®é¢˜">debian ä½¿ç”¨ dnscrypt-proxy å 53 ç«¯å£è¢«  1/init systemd å ç”¨é—®é¢˜</span><span class="categories" href="/2024/06/20/240620/">jottings</span></div></a><a class="aside-list-item" href="/2024/06/19/240619/" title="HTTP Status 307ç¼“å­˜çš„å¤„ç†"><div class="thumbnail"><img alt="HTTP Status 307ç¼“å­˜çš„å¤„ç†" src= "/img/loading.gif" data-lazy-src="/img/default.png"></div><div class="content"><span class="title" href="/2024/06/19/240619/" title="HTTP Status 307ç¼“å­˜çš„å¤„ç†">HTTP Status 307ç¼“å­˜çš„å¤„ç†</span><span class="categories" href="/2024/06/19/240619/">jottings</span></div></a></div></div></div></div></main><footer id="footer"><div id="st-footer-bar"><div class="footer-logo"><span>ä¸»é¡µ</span></div><div class="footer-bar-description">æ¥è‡ª Shane Hu çš„æ–‡ç« </div><a class="footer-bar-link" href="/about/">äº†è§£æ›´å¤š</a></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div class="copyright">Â© 2024 - 2025 By&nbsp;<a class="footer-bar-link" href="/"><img class="author-avatar" src= "/img/loading.gif" data-lazy-src="/img/pwa/favicon.png">Shane Hu</a></div><div class="beian-group"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://hexo.io/">Framework: Hexo</a><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/everfu/hexo-theme-solitude">Theme: Solitude</a></div></div></div></div><div class="comment-barrage needEndHide"></div></footer></div><!-- right_menu--><!-- inject body--><div><script src="/js/utils.js?v=3.0.14"></script><script src="/js/main.js?v=3.0.14"></script><script src="/js/third_party/waterfall.min.js?v=3.0.14"></script><script src="https://fastly.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="https://fastly.jsdelivr.net/npm/qrcodejs@1.0.0/qrcode.min.js"></script><script src="https://fastly.jsdelivr.net/npm/typeit@8.8.7/dist/index.umd.min.js"></script><script src="/js/third_party/universe.min.js?v=3.0.14"></script><script>dark()
</script><script src="/js/tw_cn.js?v=3.0.14"></script><script src="https://fastly.jsdelivr.net/npm/katex@0.16.21/dist/contrib/copy-tex.min.js"><script>(() => {
  document.querySelectorAll('.article-container span.katex-display').forEach(item => {
    utils.wrap(item, 'div', {class: 'katex-wrap'})
  })
})();
</script></script><script src="https://fastly.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><script src="https://fastly.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.js"></script><script>window.paceOptions = {
  restartOnPushState: false
}

utils.addGlobalFn('pjaxSend', () => {
  Pace.restart()
}, 'pace_restart')
</script><script src="https://fastly.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><div class="js-pjax"><script>(() => {
    let walineInitFunction = window.walineFn || null

    function initWaline(initFn) {
        const walineOptions = {
            el: '#waline-wrap',
            serverURL: '',
            pageview: false,
            dark: 'html[data-theme="dark"]',
            path: window.location.pathname,
            comment: true,
            ...null
        }
        const walineInstance = initFn(walineOptions)
        utils.addGlobalFn('pjax', () => walineInstance.destroy(), 'destroyWaline')
        GLOBAL_CONFIG.lightbox && utils.lightbox(document.querySelectorAll('#comment .wl-content img:not(.wl-emoji)'))
        sco.owoBig({
            body: '.wl-emoji-popup',
            item: '.wl-tab-wrapper button'
        })
    }

    async function loadWaline() {
        if (walineInitFunction) initWaline(walineInitFunction)
        else {
            await utils.getCSS('https://fastly.jsdelivr.net/npm/@waline/client@3.4.3/dist/waline.min.css')
            const {init} = await import('https://fastly.jsdelivr.net/npm/@waline/client@3.4.3/dist/waline.min.js')
            walineInitFunction = init || Waline.init
            initWaline(walineInitFunction)
            window.walineFn = walineInitFunction
        }
        true && barrageWaline()
    }

    if (true || true) {
        if (true) utils.loadComment(document.getElementById('waline-wrap'), loadWaline)
        else loadWaline()
    } else window.loadTwoComment = loadWaline
})()
</script><script>async function barrageWaline() {
    const url = new URL('/api/comment')
    const params = {path: window.location.pathname, sortBy: 'insertedAt_asc'}
    Object.entries(params).forEach(([key, value]) => url.searchParams.append(key, value))
    await fetch(url).then(async res => {
        if (!res.ok) throw new Error(`HTTP error! status: ${res.status}`)
        const data = await res.json();
        const regex = /<img [^>]*class="wl-emoji"[^>]*>/;
        const init = () => {
            initializeCommentBarrage(data.data.data
                .map(item => ({
                    nick: item.nick,
                    mailId: item.avatar,
                    content: item.comment.replace(regex, ''),
                    id: item.objectId
                })))
        }
        if (typeof initializeCommentBarrage === "undefined") await utils.getScript('/js/third_party/barrage.min.js?v=3.0.14').then(init)
        else init()
    }).catch(error => console.error("An error occurred while fetching comments: ", error))
}</script></div></div><!-- pjax--><script>const pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: ["title","#body-wrap","#site-config","meta[name=\"description\"]",".js-pjax","meta[property^=\"og:\"]","#config-diff",".rs_show",".rs_hide"],
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
})

document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
})

document.addEventListener('pjax:complete', () => {
    window.refreshFn()

    document.querySelectorAll('script[data-pjax]').forEach(item => {
        const newScript = document.createElement('script')
        const content = item.text || item.textContent || item.innerHTML || ""
        Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
        newScript.appendChild(document.createTextNode(content))
        item.parentNode.replaceChild(newScript, item)
    })

    GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

})

document.addEventListener('pjax:error', (e) => {
    if (e.request.status === 404) {
        pjax.loadUrl('/404.html')
    }
})</script><!-- google adsense--><!-- search--><!-- music--></body></html>
        <script>
            const posts = ["2025/03/06/250306/","2025/03/03/transformer1/","2024/06/20/2406202/","2024/06/20/240620/","2024/06/19/240619/","2024/06/06/Dante/","2024/06/06/solitude/","2024/06/05/udesk2/","2024/06/05/udesk1/","2024/06/04/Advanced-Algebra/","2024/06/04/server/","2024/06/04/font/","2024/06/04/ydns/","2024/06/04/conda/","2024/06/04/bind9/","2024/06/04/openwrt/","2024/06/04/ssl/","2024/06/03/hexo/"];
            function toRandomPost() {
                const randomPost = posts[Math.floor(Math.random() * posts.length)];
                pjax.loadUrl(GLOBAL_CONFIG.root + randomPost);
            }
        </script>